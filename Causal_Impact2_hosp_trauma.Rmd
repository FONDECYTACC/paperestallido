---
title: "Dataset Consolidation"
output:
  html_document: 
    code_folding: hide
    fig_height: 6
    fig_width: 8
    theme: spacelab
    toc: yes
    toc_depth: 6
    toc_float: yes
    number_sections: yes
---

```{=html}
<style type="text/css">
.tablelines table, .tablelines td, .tablelines th {
  border: 1px solid black;
  }
.centrado {
  text-align: center;
}
.table.center {
  margin-left:auto; 
  margin-right:auto;
}
.table_wrapper{
  display: block;
  overflow-x: auto;
  white-space: nowrap;
}
code.r{
  font-size: 8px;
}
body{ /* Normal  */
    text-align: justify;
}
.superbigimage{
  overflow-y:scroll;
  white-space: nowrap;
}
.superbigimage img{
  overflow-y: scroll;
  overflow-x: hidden;
}
p.comment {
  background-color: #FF7F79;
    padding: 10px;
  border: 1px solid black;
  margin-left: 25px;
  border-radius: 5px;
  font-style: italic;
}
</style>
```
```{=html}
<style>
  p.comment {
    background-color: #ff9a9a;
      padding: 10px;
    border: 1px solid red;
    margin-left: 25px;
    border-radius: 5px;
    font-style: italic;
  }

</style>
```
```{r setup0, include=FALSE}
rm(list=ls());gc()
unlink('Causal_Impact2_hosp_trauma_cache', recursive = TRUE)

load(paste0(getwd(),"/","Procesos hasta 4_2.RData"))
#xaringan::inf_mr()

if(isTRUE(getOption('knitr.in.progress'))==T){
    clus_iter=5000
} else {
  input <- readline('¿Are you gonna run the dataset with the whole iterations? (Si/No): ')
  if(input=="Si"){
    clus_iter=10000
  } else {
    clus_iter=1000
  }
}
```

```{r setup, include=T, message=F, warning=F}
#arriba puse algunas opciones para que por defecto escondiera el código
#también cargue algunos estilo .css para que el texto me apareciera justificado, entre otras cosas.
local({r <- getOption("repos")
       r["CRAN"] <- "http://cran.r-project.org" 
       options(repos=r)
})

`%>%` <- magrittr::`%>%`
copy_names <- function(x,row.names=FALSE,col.names=TRUE,dec=",",...) {
  if(class(ungroup(x))[1]=="tbl_df"){
        if(options()$OutDec=="."){
            options(OutDec = dec)
            write.table(format(data.frame(x)),"clipboard",sep="\t",row.names=FALSE,col.names=col.names,...)
            options(OutDec = ".")
          return(x)
        } else {
            options(OutDec = ",")
            write.table(format(data.frame(x)),"clipboard",sep="\t",row.names=FALSE,col.names=col.names,...)
            options(OutDec = ",")
          return(x)    
        }
  } else {
        if(options()$OutDec=="."){
            options(OutDec = dec)
            write.table(format(x),"clipboard",sep="\t",row.names=FALSE,col.names=col.names,...)
            options(OutDec = ".")
          return(x)
        } else {
            options(OutDec = ",")
            write.table(format(x),"clipboard",sep="\t",row.names=FALSE,col.names=col.names,...)
            options(OutDec = ",")
          return(x)       
  }
 }
}  

unlink('Causal_Impact2_hosp_trauma_cache', recursive = TRUE)
if(!require(pacman)){install.packages("pacman")}

pacman::p_unlock(lib.loc = .libPaths()) #para no tener problemas reinstalando paquetes
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
#dejo los paquetes estadísticos que voy a utilizar

if(!require(plotly)){install.packages("plotly")}
if(!require(lubridate)){install.packages("lubridate")}
if(!require(htmlwidgets)){install.packages("htmlwidgets")}
if(!require(tidyverse)){install.packages("tidyverse")}
if(!require(gganimate)){install.packages("gganimate")}
if(!require(readr)){install.packages("readr")}
if(!require(stringr)){install.packages("stringr")}
if(!require(data.table)){install.packages("data.table")}
if(!require(DT)){install.packages("DT")}
if(!require(ggplot2)){install.packages("ggplot2")}
if(!require(lattice)){install.packages("lattice")}
if(!require(forecast)){install.packages("forecast")}
if(!require(zoo)){install.packages("zoo")}
if(!require(panelView)){install.packages("panelView")}
if(!require(janitor)){install.packages("janitor")}
if(!require(rjson)){install.packages("rjson")}
if(!require(estimatr)){install.packages("estimatr")} 
if(!require(CausalImpact)){install.packages("CausalImpact")}
if(!require(textreg)){install.packages("textreg")}
if(!require(sjPlot)){install.packages("sjPlot")}
if(!require(foreign)){install.packages("foreign")}
if(!require(tsModel)){install.packages("tsModel")}
if(!require(lmtest)){install.packages("lmtest")}
if(!require(Epi)){install.packages("Epi")}
if(!require(splines)){install.packages("splines")}
if(!require(vcd)){install.packages("vcd")}
if(!require(astsa)){install.packages("astsa")}
if(!require(forecast)){install.packages("forecast")}
if(!require(MASS)){install.packages("MASS")}
if(!require(ggsci)){install.packages("ggsci")}
if(!require(Hmisc)){install.packages("Hmisc")}
if(!require(compareGroups)){install.packages("compareGroups")}
if(!require(dplyr)){install.packages("dplyr")}
if(!require(ggforce)){install.packages("ggforce")}
if(!require(imputeTS)){install.packages("imputeTS")}
if(!require(doParallel)){install.packages("doParallel")}
if(!require(SCtools)){install.packages("SCtools")}
if(!require(MSCMT)){install.packages("MSCMT")}
# Calculate the number of cores
no_cores <- detectCores() - 1
cl<-makeCluster(no_cores)
registerDoParallel(cl)

Sys.setlocale(category = "LC_ALL", locale = "english")
```

# Exploration of Time Series of Trauma Hospitalizations


We explored the weekly trends in each year.

<br>

```{r exp_plot_lines, echo=T, cache= T, paged.print=TRUE, warning=F, fig.height=14,eval=T, fig.align='center', fig.cap= "Figure 1. Linear trends of Weekly Trauma Hospitalizations by Years (Median and IQRs)"}
data15a64_rn%>%
  dplyr::mutate(year_week_fmt=as.Date(year_week,"%Y-W%V"))%>%
 # dplyr::group_by(year,yearweek)%>%
  #dplyr::select(hosp_trauma,hosp_circ,hosp_resp) %>%  summary(min(),max())
  #dplyr::summarise(median=median(hosp_trauma,na.rm=T))%>%
ggplot() + #median
  #geom_point() + 
  #geom_line() +
  geom_jitter(aes(y = hosp_trauma,x = isoweek, color = year),width = 0.5, alpha = 0.3)+
  facet_wrap(~year, ncol = 1) + 
  theme_bw() + 
  ylab("Trauma Hospitalizations") + 
  xlab("Week") + 
  theme(strip.text.x = element_text(size = 14, face = "bold"),
        legend.position = "none",
        plot.caption=element_text(hjust = 0))+
  geom_smooth(aes(y = hosp_trauma,x = isoweek, color = year),stat = 'summary', color = 'red', fill = 'red', alpha = 0.2, 
                fun.data = median_hilow, fun.args = list(conf.int = 0.5))+
  geom_smooth(aes(y = hosp_circ,x = isoweek, color = year),stat = 'summary', color = 'darkblue', fill = 'navyblue', alpha = 0.2, 
                fun.data = median_hilow, fun.args = list(conf.int = 0.5))+
  geom_smooth(aes(y = hosp_resp,x = isoweek, color = year),stat = 'summary', color = 'violet', fill = 'violet', alpha = 0.2, 
                fun.data = median_hilow, fun.args = list(conf.int = 0.5))+
  #geom_smooth(aes(y = cons_trauma,x = yearweek, color = year),stat = 'summary', color = 'darkgreen', fill = 'green', alpha = 0.2, 
  #              fun.data = median_hilow, fun.args = list(conf.int = 0.5))+
    #  geom_smooth(aes(y = hosp_total,x = yearweek, color = year),stat = 'summary', color = 'gray80', fill = 'gray80', alpha = 0.2, 
   #             fun.data = median_hilow, fun.args = list(conf.int = 0.5))+
  labs(caption="Note. Year 2015 & 2016 had a length of 53 weeks.\nRed= Trauma Hospitalizations; Blue= Circulatory System Hospitalizations; Violet= Respiratory System Hospitalizations; Gray Dots= Trauma Hospitalizations (obs)")+
  
  scale_y_continuous(breaks=seq(0,1000,50))+
  scale_x_continuous(
  breaks = seq(from = 1, to = 53, by =4)#,
#  label = c("two", "four", "six")
)
```

```{r setting_prev_a_ts_set, echo=T, cache= T, paged.print=TRUE, warning=F, eval=T}
#number of times that data was collected per year by using the frequency parameter in the ts( )

tsData1 <- ts(data15a64_rn$hosp_trauma, frequency=1) #a cada unidad temporal le asigno una observación, por lo que asume que la unidad temporal considerada es el día
tsData4 <- ts(data15a64_rn$hosp_trauma, frequency=4) #a cada unidad temporal le asigno una observación, por lo que asume que la unidad temporal considerada es el día
tsData52.4 <- ts(data15a64_rn$hosp_trauma, frequency=52.4)#a cada unidad temporal le asigno una observación, por lo que asume que la unidad temporal considerada es la semana (262) #53+53+52+52+52
tsData17.5 <- ts(data15a64_rn$hosp_trauma, frequency=17.5)#a cada unidad temporal le asigno una observación, por lo que asume que la unidad temporal considerada es la semana (262) #53+53+52+52+52
tsData_m <-msts(data15a64_rn$hosp_trauma, seasonal.periods=c(4,52.5)) #a cada unidad temporal le asigno una observación, por lo que asume que la unidad temporal considerada es el día
#tsData48 <- ts(data15a64_rn$hosp_trauma, frequency=48)
#tsData262 <- ts(data15a64_rn$hosp_trauma, frequency=data15a64%>%dplyr::mutate(concat=paste0(year,"_",yearweek))%>%distinct(concat)%>% nrow())

gg0 <- list()
gg0[1] <- "tsData4"
gg0[2] <- "tsData17.5"
gg0[3] <- "tsData52.4"
invisible(c("frequency :the number of observations per “cycle” (normally a year, but sometimes a week, a day or an hour). "))
invisible(c("The “frequency” is the number of observations before the seasonal pattern repeats."))
```

## Explore Decomposition {.tabset .tabset-fade}

<br>

We explored several additive and multiplicative decomposition of the data, depending on the number of observations assigned to each time point. We decomposed the time series in three alternatives: one assigning 4 obs. to each temporal unity (as Monthly Series), a second assigning 17.5 obs. (as Quarterly Series) to each temporal unity, and another assumed 52.4 obs. to each temporal unity (as Yearly Series).

<br>

```{r setting_prev_a_decomp, echo=T, cache= T, paged.print=TRUE, warning=F, eval=T,results='asis'}
#https://otexts.com/fpp2/decomposition.html
headings <- c('=4obs.', '=17.5obs.', '=52.4obs.','=Multiple(4 & 52.4)')

for (i in 1:length(gg0)) {
  cat("### ",headings[i],"\n")
  plot(decompose(get(gg0[[i]])))
  plot(decompose(get(gg0[[i]]), type="multiplicative"))
  cat('\n\n')
}

#https://pkg.robjhyndman.com/forecast/reference/mstl.html
cat("### ",headings[4],"\n")
mstl(tsData_m, lambda = NULL) %>% autoplot()+
  theme_bw()

#Aditivo: Seasonal+Trend+Random
#Multiplicativo:Seasonal*Trend*Random
#El multiplicativo asume que la interacción no es constante ni los componentes son consistentes
#
## Observed: los datos actuales
## Trend: el movimiento general hacia arriba o hacia abajo de los puntos de datos
## Seasonal: cualquier patrón mensual / anual de los puntos de los datos
## Random – parte inexplicable de los datos

#Determinar
## Tendencia: a largo plazo
## Estacional/Periodico: cuando la serie está influenciada por un periodo fijo y conocido (dia mes o semana)
## Cíclico= cuando hay subidas y caidas que no corresponden a un periodo fio
```

<br>

## Correlograms

<br>

The visual inspection of the spectral characteristics of the series permitted us to appreciate an important upward trend, and a seasonality determined by yearly combined with weekly cycles.

<br>

```{r setting_prev_b_ACF, echo=T, cache= T, paged.print=TRUE, warning=F, eval=T, fig.align='center', fig.cap= "Figure 3. Correlograms of Series"}
#acf() calcula la función de autocorrelación simple de una serie temporal, y pacf() la función de autocorrelación parcial. En ambos casos, por defecto se muestra el gráfico con bandas de confianza al 95%.
ggAcf(data15a64_rn$hosp_trauma,20,main="3.a.Autocorreation Plot of the Series of Trauma Hospitalizations")+ #data15a64_rn$hosp_trauma
  theme_sjplot()+
  theme(plot.caption=element_text(hjust = 0))+
  labs(x="Number of Lags",y="ACF",caption="Note. Dotted Blue Line= 95% CI")
lags_acf<-ggAcf(tsData1,20)$data %>% dplyr::arrange(desc(abs(Freq)))%>% slice(1:10)%>% dplyr::select(lag)
ggPacf(data15a64_rn$hosp_trauma,20,main="3.b.Partial Autocorreation Plot of the Series of Trauma Hospitalizations")+ #data15a64_rn$hosp_trauma
  theme_sjplot()+
  theme(plot.caption=element_text(hjust = 0))+
  labs(x="Number of Lags",y="ACF",caption="Note. Dotted Blue Line= 95% CI")
lags_pacf<-ggPacf(tsData1,20)$data %>% dplyr::arrange(desc(abs(Freq)))%>% slice(1:10)%>% dplyr::select(lag)

#tseries::adf.test(tsData)
#Augmented Dickey-Fuller Test gives a p-value of 0.01, so we have enough evidence to reject null hypothesis of non-stationarity.

#The test statistic is much bigger than the 1% critical value, indicating that the null hypothesis is rejected. That is, the data are not stationary. We can difference the data, and apply the test again.


#Se requiere para que las estimaciones de los parámetros sean útiles. De otra forma, no se podrían calcular medias y variancias conforme la serie va creciendo

#LOS MODELOS ARCH Y GARCH NO REQUIEREN CONSISTENCIA EN LA VARIANZA
#eing able to control the lags in our test, allows us to avoid a stationarity test that is too complex to be supported by our data.

#https://nwfsc-timeseries.github.io/atsa-labs/sec-boxjenkins-aug-dickey-fuller.html
 
#todos los parámetros son sig., por lo que son eestacionarios
```

<br>

As seen in the correlograms and in the multiple decompositions, the series contains an upward trend, leading to think that the mean was not constant over time, and there could be evidence of seasonallity. This could be interpreted as that the series were not stationary. However, the Dickey-Fuller test indicated the presence of stationarity (`r round(tseries::adf.test(tsData1)$statistic,2)`, p\<`r round(tseries::adf.test(tsData1)$p.value,3)`).

<br>

## Time-series models & Back-testing approach

<br>

We decided to automatically select the model with the better adjustment to the pre-intervention periods.

<br>

```{r setting_prev_c_compare_models, echo=T, cache= T, paged.print=TRUE, warning=F, eval=T, fig.align='center', fig.cap= "Figure 4. Comparison of Forecasting Methods, in Mean Square Error (MSE)"}
fets <- function(x, h) {
  forecast(ets(x), h = h)
}
farima <- function(x, h) {
  forecast(auto.arima(x), h=h)
}
fsnaive <- function(x, h) {
  forecast(snaive(x), h=h)
}

# Compute CV errors for ETS as e1
e1 <- tsCV(data15a64_rn[which(data15a64_rn$did==0),"hosp_trauma"], fets, h=rn_summ_data15a64$diff)
# Compute CV errors for ARIMA as e2
e2 <- tsCV(data15a64_rn[which(data15a64_rn$did==0),"hosp_trauma"], farima, h=rn_summ_data15a64$diff)

e3 <- tsCV(data15a64_rn[which(data15a64_rn$did==0),"hosp_trauma"], naive, h=rn_summ_data15a64$diff)

e4 <- tsCV(data15a64_rn[which(data15a64_rn$did==0),"hosp_trauma"], ses, h=rn_summ_data15a64$diff)

e5 <- tsCV(data15a64_rn[which(data15a64_rn$did==0),"hosp_trauma"], holt, h=rn_summ_data15a64$diff)

e6 <- tsCV(data15a64_rn[which(data15a64_rn$did==0),"hosp_trauma"], holt, seasonal="additive", damped=T, h=rn_summ_data15a64$diff)

mse1 <- colMeans(e1^2, na.rm = TRUE)
mse2 <- colMeans(e2^2, na.rm = TRUE)
mse3 <- colMeans(e3^2, na.rm = TRUE)
mse4 <- colMeans(e4^2, na.rm = TRUE)
mse5 <- colMeans(e5^2, na.rm = TRUE)
mse6 <- colMeans(e6^2, na.rm = TRUE)

h<- 1:rn_summ_data15a64$diff
mse_comb<- data.frame(cbind(h,mse1,mse2,mse3,mse4,mse5,mse6))

mse_comb%>% 
    melt(id=1)%>%
    ggplot(aes(x = h, y = value,shape=variable, color=variable))+
    geom_point(size=4)+
    #geom_line(aes(linetype=variable))+
    theme_sjplot()+
    labs(x="Forecast Horizon",y="MSE",caption=paste0("Note. "),color  ="Forecasting\nMethods", shape = "Forecasting\nMethods")+   
    scale_color_manual(values=c("#999999", "midnightblue","#E69F00", "darkred","salmon","darkslategrey"), 
                       name="Forecasting\nMethods",
                       #breaks=c("ctrl", "trt1", "trt2"),
                       labels=c("ETS", "ARIMA", "Naive","SES","Holt","HW-add","HW-mult"))+
      scale_shape_manual(values=c(0:6)+15,name="Forecasting\nMethods")+
    guides(colour = guide_legend(override.aes = list(shape = c(0:5)+15, color = c("#999999", "midnightblue","#E69F00", "darkred","salmon","darkslategrey"))),shape="none")+
    scale_x_continuous(limits=c(1,rn_summ_data15a64$diff),breaks=seq(1,rn_summ_data15a64$diff,by=1))

#ARIMA models
###some are stationary
###do not have exponential smoothing counterparts
###use if you see autocorrelation in the data, i.e. the past data explains the present data well

#ETS models
#ETS(A, N, N): Simple exponential smoothing with additive errors 'A'/'M' stands for whether you add the errors on or multiply the errors on the point forecsats
###are not stationary
###use exponential smoothing
###use if there is a trend and/or seasonality in the data, as this model explicitly models these components
##Automatically chooses a model by default
##Can handle any combination of trend, seasonality and damping
##Produces prediction intervals for every model
##Ensures the parameters are admissible (equivalent to invertible)
##Produces an object of class ets

ets1<-ets(tsData1)
#round(ets1$aicc,0)
#Myth that ARIMA models are more general than exponential smoothing.
#Linear exponential smoothing models all special cases of ARIMA models.
#Non-linear exponential smoothing models have no equivalent ARIMA counterparts.
#Many ARIMA models have no exponential smoothing counterparts.
#ETS models all non-stationary. Models with seasonality or non-damped trend (or both) have two unit roots; all other models have one unit root.

#your data is integer, then applying ARIMA models might not be appropriate
```

<br>

Exponential smoothing and ARIMA models are the two most widely-used approaches to time series forecasting, and provide complementary approaches to the problem

<br>

We computed the cross-validation errors of different methods, using data from the pre-intervention period. The first method, the Naive (`r round(mean(e3^2, na.rm=TRUE),2)`) had the greatest errors, while exponential smoothing methods (Error Trend and Seasonality, ETS) -that identified multiplicative errors but no trend nor seasonallity (`r as.character(ets1$method)`)- shown greater average error (MSE=`r round(mean(e1^2, na.rm=TRUE),2)`), than the SES (Simple Exponential Smoothing) (`r round(mean(e4^2, na.rm=TRUE),2)`) and ARIMA methods (MSE= `r round(mean(e2^2, na.rm=TRUE),2)`). This is why we proceeded with the ARIMA models.

<br>

Since we lack of experience fitting time series models manually, the optimal model selected is the one with the better adjusted indices.

<br>

```{r setting_prev_d, echo=T, cache= T, paged.print=TRUE, warning=F, eval=T}
#x1<-as.numeric(ts_data15a64_rn[,"offset"])
#Decisión de ocupar automáticos: https://robjhyndman.com/talks/MelbourneRUG.pdf

#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_
#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_

# Autocorrelación= p d q para modelos ARIMA
# Análisis espectral= comportamiento cíclico, separando componentes ruidosos
# Estimación y descomposición= ajuste estacional, serie de c componentes con características

#p= 	order of the autoregressive part;
#d=  	degree of first differencing involved;
#q= 	order of the moving average part.

tsData <- ts(data15a64_rn$hosp_trauma, frequency=1) #a cada unidad temporal le asigno una observación, por lo que asume que la unidad temporal considerada es el día

#https://nbviewer.jupyter.org/github/dafiti/causalimpact/blob/master/examples/getting_started.ipynb
#https://stackoverflow.com/questions/30303680/how-to-impose-restrictions-on-predictions-made-using-a-bayesian-structural-time

#ets(zoo(data.frame(data15a64_rn[,"hosp_trauma"])))
ARIMA_fit<-auto.arima(data.frame(data15a64_rn[1:post.period[1],"hosp_trauma"]), trace=TRUE,stepwise = F)#, method='CSS')
```

<br>

The selected model with the best indices was the ARIMA(1, 1, 2), which means that it you are describing some response variable (Y) by combining a 1st order Auto-Regressive model and a 2nd order Moving Average model. The 'I' (differencing) part of the model (the Integrative part) it signifies a model where we were not taking the difference between response variable data: non-stationary data of 1 (MA moving average) (AICc=`r round(ARIMA_fit$aicc)`;BIC= `r round(ARIMA_fit$bic,2)`).

<br>

```{r setting_prev_d_plot_fitted_vs_actual, echo=T, cache= T, paged.print=TRUE, warning=F, eval=T, fig.align='center', fig.cap= "Figure 5. Actual linear trend vs. Fitted Line (ARIMA 1,1,2)",fig.height=12}
#will be to fit a model to the data, and then use the forecast() function to produce forecasts from that model
data_forecast_arima112<-forecast(ARIMA_fit, h=rn_summ_data15a64$diff,bootstrap=5000)
#ts(data15a64_rn[1:post.period[1],"hosp_trauma"]) %>% ets() %>% forecast(h=rn_summ_data15a64$diff,bootstrap=5000) %>% autoplot()

fitted_and_forecast_arima<-c(as.numeric(data_forecast_arima112$fitted),as.numeric(data_forecast_arima112$mean))
actual_vs_fitted_and_forecast_arima<-cbind(actual=data15a64_rn[,c("date","did","rn","hosp_trauma")],
                                           arima=data.frame(fitted_and_forecast_arima),
                                           lo_80=c(rep(NA,post.period[1]),as.numeric(data_forecast_arima112$lower[,1])),
                                           lo_95=c(rep(NA,post.period[1]),as.numeric(data_forecast_arima112$lower[,2])),
                                           up_80=c(rep(NA,post.period[1]),as.numeric(data_forecast_arima112$upper[,1])),
                                           up_95=c(rep(NA,post.period[1]),as.numeric(data_forecast_arima112$upper[,2])))

### MAPE (mean absolute percentage error)
MAPE_prev <- actual_vs_fitted_and_forecast_arima%>%
  #dplyr::mutate(actual.date=as.Date(actual.year_week))%>%
  dplyr::filter(actual.hosp_trauma!=0)%>% #had 0 2019-08-06 
  dplyr::filter(actual.did==0)%>%
  summarise(MAPE=mean(abs(actual.hosp_trauma-fitted_and_forecast_arima)/actual.hosp_trauma,na.rm=T))

MAPE_post <- actual_vs_fitted_and_forecast_arima%>%
  dplyr::filter(actual.did==1)%>%
  summarise(MAPE=mean(abs(actual.hosp_trauma-fitted_and_forecast_arima)/actual.hosp_trauma))

post_did<-actual_vs_fitted_and_forecast_arima[which(actual_vs_fitted_and_forecast_arima$actual.did ==1),]

lenfinal<-NULL
for (i in 15:19){lenfinal<-c(lenfinal,paste0("Q",1:4,"\n",i))}

actual_vs_fitted_and_forecast_arima%>%
  dplyr::mutate(actual.date=as.Date(actual.date))%>%
ggplot()+
  geom_line(aes(x=actual.rn, y=actual.hosp_trauma), color="grey30")+
  geom_line(aes(x=actual.rn, y=fitted_and_forecast_arima), color="red", size=1)+
  geom_vline(xintercept =253, linetype="dotted", color = "blue", size=1.5)+
  geom_ribbon(aes(x= actual.rn, ymin = lo_80, ymax = up_80), fill = "brown", alpha = .6)+
  geom_ribbon(aes(x= actual.rn, ymin = lo_95, ymax = up_95), fill = "red", alpha = .3)+
  theme_sjplot()+
  theme(plot.caption=element_text(hjust = 0))+
  labs(x="Date",y="Trauma Hospitalizations",caption=paste0("Note. Red line= Fitted line; Gray lines= Actual Trauma Hospitalizations;\nVertical line= 2020-10-18;\n",paste0("MAPE  in post-intervention period=",round(100*MAPE_post,1),"%")))+
  xlab("")+
  theme(legend.title=element_blank(),
        axis.text.x=element_text(size=10))+
  scale_x_continuous(
  breaks = seq(from = 1, to = nrow(actual_vs_fitted_and_forecast_arima), by =12),
  label = actual_vs_fitted_and_forecast_arima[which(actual_vs_fitted_and_forecast_arima$actual.rn %in% seq(from = 1, to = nrow(actual_vs_fitted_and_forecast_arima), by =12)),"actual.date"])+
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=.5))+
  facet_zoom(xlim = c(253, nrow(actual_vs_fitted_and_forecast_arima)))

  #ggforce::facet_zoom(x = actual.date>=as.Date("2019-10-18") & actual.date<as.Date("2019-12-31"))
```

<br>

As seen in the Figure, the model seemed to fit well, however it shows a MAPE (mean absolute percentage error) of `r paste0(round(100*MAPE_prev,1),"%")` in the pre-intervention period.

<br>

### Characteristics of the ARIMA model selected

We checked the ARIMA model, residuals and autocorrelations of the model. The following plot shows a general overview of the characteristics of the model.

<br>

```{r setting_prev_e_check_resid1_general, echo=T, cache= T, paged.print=TRUE, warning=F, eval=T, fig.align='center', fig.cap= "Figure 6. Residuals of the Model"}
checkresiduals(ARIMA_fit)
#ARIMA(2,1,2); #ARIMA(1,1,2); #ARIMA(2,1,2); # ARIMA(2,1,3)
```

<br>

The selected model was the most parsimonious one among the possible candidates using `auto.arima` function, and the mean seem to be close to 0 (`r paste0("M= ",round(mean(ARIMA_fit$residuals),3))`), but we could observe that errors may not be distributed independently `r paste0("(W=",as.numeric(round(shapiro.test(ARIMA_fit$residuals)$statistic,3)),"; p.value=",as.numeric(round(shapiro.test(ARIMA_fit$residuals)$p.value,3)),")")`. However, residuals were not significantly different from white noise series (`r paste0("Chi-square ","(",Box.test(ARIMA_fit$residuals,lag=10, fitdf=0, type="Lj")$df,")=",round(Box.test(ARIMA_fit$residuals,lag=10, fitdf=0, type="Lj")$statistic,2)," p=",round(Box.test(ARIMA_fit$residuals,lag=10, fitdf=0, type="Lj")$p.value,3))`).

Below we may see a detailed figure of autocorrelation and partial autocorrelation plots.

<br>

```{r setting_prev_e_check_resid2, echo=T, cache= T, paged.print=TRUE, warning=F, eval=T, fig.align='center', fig.cap= "Figure 7. Correlograms of Residuals"}
invisible(c("IGUAL LOS RESIDUOS NO SE DISTIRBUYEN NORMALMENTE!!!!!!!!!!!!!!!!!!!!!!!!!"))

#One simple "sanity check" would be to find series where your forecast is higher than the highest historical observation, or higher than the historical 90% quantile. This often signals trouble, except possibly for very new products that are ramping up.
#Lag 6, 7 y 18 tienen spikes. 
# El residuo no es ruido blanco

#acf() calcula la función de autocorrelación simple de una serie temporal, y pacf() la función de autocorrelación parcial. En ambos casos, por defecto se muestra el gráfico con bandas de confianza al 95%.
ggAcf(ARIMA_fit$residuals,20,main="6.a.Autocorreation Plot of the residuals of the ARIMA model")+ #data15a64_rn$hosp_trauma
  theme_sjplot()+
  theme(plot.caption=element_text(hjust = 0))+
  labs(x="Number of Lags",y="ACF",caption="Note. Dotted Blue Line= 95% CI")+
  ylim(-.25,.25)
lags_acf<-ggAcf(ARIMA_fit$residuals,400)$data %>% dplyr::arrange(desc(abs(Freq)))%>% slice(1:10)%>% dplyr::select(lag)
ggPacf(ARIMA_fit$residuals,20,main="6.b.Partial Autocorreation Plot of the residuals of the ARIMA model")+ #data15a64_rn$hosp_trauma
  theme_sjplot()+
  theme(plot.caption=element_text(hjust = 0))+
  labs(x="Number of Lags",y="ACF",caption="Note. Dotted Blue Line= 95% CI")+
  ylim(-.25,.25)
lags_pacf<-ggPacf(ARIMA_fit$residuals,400)$data %>% dplyr::arrange(desc(abs(Freq)))%>% slice(1:10)%>% dplyr::select(lag)
 # ggforce::facet_zoom(x = lag >=5 & lag <=10)+
 # ggforce::facet_zoom(x = lag >=20 & lag <=25)
```

<br>

As shown in the autoccorelation plots, the selected model may still be correlated with some lagged values. Particularly, there were some significant correlations in the residuals series, such as with `r lags_acf[1,1]`,`r  lags_acf[2,1]`,`r lags_acf[3,1]`,`r lags_acf[4,1]`,`r lags_acf[5,1]`,`r lags_acf[6,1]`,`r lags_acf[7,1]`,`r lags_acf[8,1]`,`r lags_acf[9,1]`, and `r lags_acf[10,1]`. Focusing on indirect effects, the partial autocorrelation function also identifies autocorrelations that were greater than the correlation bar.

<br>

```{r setting_prev_f_resid_vs_fit, echo=T, cache= T, paged.print=TRUE, warning=F, eval=T, fig.align='center', fig.cap= "Figure 8. Scatter-plot of Residuals vs. Fitted ARIMA model"}
#plot(,,type="p",pch=9, col="blue",xlab="Fitted",ylab="Resid.")
#abline(h=0,lty=2,lwd=2)

cbind(Fitted = ARIMA_fit$fitted,
      Residuals=ARIMA_fit$residuals) %>%
  as.data.frame() %>%
  ggplot(aes(x=Fitted, y=Residuals)) + 
  geom_point()+
  theme_sjplot()+
  #ylim(-20,20)+
  #xlim(0,20)+
  geom_hline(aes(yintercept = 0),linetype = 2)

#tsData %>% urca::ur.kpss() %>% summary()
```

<br>

As shown in the Figure above, no pattern was found between residuals and predicted values, excepting for some outliers.

<br>

```{r setting_prev_g_tbats_multiple_time_seasonality, echo=T, cache= T, paged.print=TRUE, warning=F, eval=F, fig.cap="Figure 3. NO LO OCUPO"}
 
tsData52.4%>% tbats()
#TBATS(1, {1,1}, -, {<52.4,6>})= el primer 1 significa que no es necesario hace trasformación box-cox; los 1 y 1 significa que hay un error ARMA de 1 y 1; no hay amortiguación/damping--> lo final dice de los términos fourier; se seleccionaron 6
#no se ven muchos retrasos

tsData_m%>% tbats()%>% forecast()%>% autoplot()
```


# TBATS

This models let us include multiple seasonalities. Their initials stand for T (trigonometric regressors for multiple seasonalities), B (box-cox transformations that correct for the effect of normality in data), A (inclusion of autorregresive  and moving-average errors), T (trend) y S (seasonality). This model automatically identifies the parameters that reduces. However, we pre-defined a monthly and annual seasonality, in order to capture monthly and annual seasonal patterns.

<br>

The test set were defined 11 weeks before the intervention period (week of October 21th, 2019). The training set had information of each outcome from 2015 to the first semester of 2019. **We expect that these models would give us information about the structure of the time series that will be used in the Bayesian Time-Series Analysis.

<br>

```{r setting_prev_g_tbats_multiple_time_seasonality, echo=T, cache= T, paged.print=TRUE, warning=F, eval=T}
pre_period_response_hosp_trauma <- as.numeric(unlist(data15a64_rn[which(data15a64_rn$did==0),"hosp_trauma"]))
pre_period_response_hosp_resp <- as.numeric(unlist(data15a64_rn[which(data15a64_rn$did==0),"hosp_resp"]))
pre_period_response_cons_resp <- as.numeric(unlist(data15a64_rn[which(data15a64_rn$did==0),"cons_resp"]))
pre_period_response_cons_trauma <- as.numeric(unlist(data15a64_rn[which(data15a64_rn$did==0),"cons_trauma"]))

#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:

pre_period_response_hosp_trauma_train<-as.numeric(unlist(data15a64_rn[1:241,"hosp_trauma"]))%>% 
                        ts()
pre_period_response_hosp_trauma_val<-as.numeric(unlist(data15a64_rn[242:252,"hosp_trauma"]))%>% 
                        ts(start=242)

pre_period_response_hosp_resp_train<-as.numeric(unlist(data15a64_rn[1:241,"hosp_resp"]))%>% 
                        ts()
pre_period_response_hosp_resp_val<-as.numeric(unlist(data15a64_rn[242:252,"hosp_resp"]))%>% 
                         ts(start=242)

pre_period_response_cons_resp_train<-as.numeric(unlist(data15a64_rn[1:241,"cons_resp"]))%>% 
                        ts()
pre_period_response_cons_resp_val<-as.numeric(unlist(data15a64_rn[242:252,"cons_resp"]))%>% 
                         ts(start=242)

pre_period_response_cons_trauma_train<-as.numeric(unlist(data15a64_rn[1:241,"cons_trauma"]))%>% 
                        ts()
pre_period_response_cons_trauma_val<-as.numeric(unlist(data15a64_rn[242:252,"cons_trauma"])) %>% 
                        ts(start=242) 

#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:
#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:
set.seed(2125)
fit_trainset_tbats_hosp_trauma <- tbats(pre_period_response_hosp_trauma_train,seasonal.periods= c(52,4),biasadj=T)
fit_trainset_tbats_hosp_resp <- tbats(pre_period_response_hosp_resp_train,seasonal.periods= c(52,4),biasadj=T)
fit_trainset_tbats_cons_trauma <- tbats(pre_period_response_cons_trauma_train,seasonal.periods= c(52,4),biasadj=T)
fit_trainset_tbats_cons_resp <- tbats(pre_period_response_cons_resp_train,seasonal.periods= c(52,4),biasadj=T)

#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:
#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:

#Trigonometric terms for seasonality
#1	Box-Cox transformation parameter, for heterogeneity
#{0,0}	ARMA error for short-term dynamics
#-	Damping parameter -> Trend
#{\<51.18,14>}	Seasonal period, Fourier terms

#Las opciones tomadas: 1(no transformación), la segunda es el error ARMA (1,1),p=1, q=1, - significa que no hay damping o trend, después vienen los periodos estacionales (no hay), y no hay serialized terms seleccionados; puede que estas dos estén en 2 grupos <> <> porque haya más estacionalidad 
#
# The TBATS incorporates a state space model that is a generalization of those underpinning exponential smoothing. It also allows for automatic Box-Cox transformation and ARMA errors, it provides a very flexible way of accounting for seasonality and trends. The parameters include a box-cox transformation parameter, ARMA errors, damping. The seasonality and trend is easily handled in this model through the fourier transformations and smoothening parameters, which can be further tuned. Due to limited data however, the accuracy on test set in terms of Mean Absolute Percentage Error can vary from the metric given in this report, however when forecasting for longer periods, we see very smooth trends. After making the forecasting model, the expense percentage predicted for May 2016 is 3.52%.
```

<br>

First, we applied the TBATS model on Trauma Hospitalizations.

<br>

```{r setting_tbats_hosp_trauma, echo=T, cache= T, fig.width=8, paged.print=TRUE, warning=F, eval=T, fig.align="center", fig.cap="Figure 1. TBATS Model of Truama Hospitalizations in the Pre-Intervention Period"}
forecast(fit_trainset_tbats_hosp_trauma, h=11) %>%  autoplot()+ autolayer(pre_period_response_hosp_trauma_val, color="red",size=1)+
  theme_bw()+
  xlab("Date")+
  ylab("Hospitalizations")+
  scale_x_continuous(breaks=(c(seq(1,nrow(data15a64_rn),12),262)),
                     labels=as.character(unlist(data15a64_rn[c(seq(1,nrow(data15a64_rn),12),262),"date"])))+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=.5,size=10),
        plot.caption = element_text(hjust = 0, face= "italic"))+
      theme(legend.position="bottom")+geom_vline(xintercept = 252, linetype="dotted", 
                color = "darkgreen", size=1.5,
        plot.caption = element_text(hjust = 0, face= "italic"))+
  labs(caption="Note. Blue Line= 11 weeks forecasted; Blue area= Prediction intervals for the Forcast period;\nRed Line= Actual Trend; Vertical Green Dotted Line= Intervention Period (Social Protests).")
#Sure, you can apply a Box-Cox Transformation, e.g. lambda = 0 (Log-Transform), on the time series before fitting the model. After which you need the apply the inverse Box-Cox transformation on predictions. In that way you ensure that the forecasts and confidence intervals are positive

#RESIDUALS
#forecast::ggtsdisplay(residuals(fit_trainset_tbats_hosp_trauma), lag.max=52, main='HoltWinters Exponenetial smoothing Residuals')
```
<br>

The model suggested a box-cox transformation of lambda=`r sprintf("%1.2f",as.numeric(fit_trainset_tbats_hosp_trauma$lambda))`, that can be rounded to the squared root of Y (√(Y)).  However, it may very well be the case that the resulting transformation of the data is not appropriate to our case. Additionally, the model did not identify ARMA errors and trends (damping).

<!--- 0= suggest a log transformation --->

<br>

Next, we applied the TBATS model on Respiratory Hospitalizations. 

<br>

```{r setting_tbats_hosp_resp, echo=T, cache= T, fig.width=8, paged.print=TRUE, warning=F, eval=T, fig.align="center", fig.cap="Figure 2. TBATS Model of Respiratory Hospitalizations in the Pre-Intervention Period"}
forecast(fit_trainset_tbats_hosp_resp, h=11) %>%  autoplot()+ autolayer(pre_period_response_hosp_resp_val, color="red",size=1)+
  theme_bw()+
  xlab("Date")+
  ylab("Hospitalizations")+
  scale_x_continuous(breaks=(c(seq(1,nrow(data15a64_rn),12),262)),
                     labels=as.character(unlist(data15a64_rn[c(seq(1,nrow(data15a64_rn),12),262),"date"])))+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=.5,size=10),
        plot.caption = element_text(hjust = 0, face= "italic"))+
      theme(legend.position="bottom")+geom_vline(xintercept = 252, linetype="dotted", 
                color = "darkgreen", size=1.5,
        plot.caption = element_text(hjust = 0, face= "italic"))+
  labs(caption="Note. Blue Line= 11 weeks forecasted; Blue area= Prediction intervals for the Forcast period;\nRed Line= Actual Trend; Vertical Green Dotted Line= Intervention Period (Social Protests).")
```

<br>

The resulting model suggested a log transformation (Lambda= `r sprintf("%1.2f",as.numeric(fit_trainset_tbats_hosp_resp$lambda))`), but did not identify ARMA errors nor trends, but seasonal components. 

<br>

Also we applied the TBATS model on Trauma Consultations. 


<br>

```{r setting_tbats_cons_trauma, echo=T, cache= T, fig.width=8, paged.print=TRUE, warning=F, eval=T,  fig.align="center",fig.cap="Figure 3. TBATS Model of Trauma Consultations in the Pre-Intervention Period"}
forecast(fit_trainset_tbats_cons_trauma, h=11) %>%  autoplot()+ autolayer(pre_period_response_cons_trauma_val, color="red",size=1)+
  theme_bw()+
  xlab("Date")+
  ylab("Consultations")+
  scale_x_continuous(breaks=(c(seq(1,nrow(data15a64_rn),12),262)),
                     labels=as.character(unlist(data15a64_rn[c(seq(1,nrow(data15a64_rn),12),262),"date"])))+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=.5,size=10),
        plot.caption = element_text(hjust = 0, face= "italic"))+
      theme(legend.position="bottom")+geom_vline(xintercept = 252, linetype="dotted", 
                color = "darkgreen", size=1.5,
        plot.caption = element_text(hjust = 0, face= "italic"))+
  labs(caption="Note. Blue Line= 11 weeks forecasted; Blue area= Prediction intervals for the Forcast period;\nRed Line= Actual Trend; Vertical Green Dotted Line= Intervention Period (Social Protests).")
```

<br>

The model identified no box-cox transformations, no ARMA errors, a trend of `r round(fit_trainset_tbats_cons_trauma$damping.parameter,1)` but no seasonal periods. However, a visual inspection may not let us to conclude that there was an upward trend on the series.

<br>

```{r setting_tbats_cons_resp, echo=T, cache= T, fig.width=8, paged.print=TRUE, warning=F, eval=T,  fig.align="center",fig.cap="Figure 3. TBATS Model of Respiratory Consultations in the Pre-Intervention Period"}
forecast(fit_trainset_tbats_cons_resp, h=11) %>%  autoplot()+ autolayer(pre_period_response_cons_resp_val, color="red",size=1)+
  theme_bw()+
  xlab("Date")+
  ylab("Consultations")+
  scale_x_continuous(breaks=(c(seq(1,nrow(data15a64_rn),12),262)),
                     labels=as.character(unlist(data15a64_rn[c(seq(1,nrow(data15a64_rn),12),262),"date"])))+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=.5,size=10),
        plot.caption = element_text(hjust = 0, face= "italic"))+
      theme(legend.position="bottom")+geom_vline(xintercept = 252, linetype="dotted", 
                color = "darkgreen", size=1.5,
        plot.caption = element_text(hjust = 0, face= "italic"))+
  labs(caption="Note. Blue Line= 11 weeks forecasted; Blue area= Prediction intervals for the Forcast period;\nRed Line= Actual Trend; Vertical Green Dotted Line= Intervention Period (Social Protests).")
```

<br>

The resulting model suggested a box-cox transformation of lambda=`r sprintf("%1.2f",as.numeric(fit_trainset_tbats_cons_resp$lambda))`, that can be rounded to 1 (no transformation needed). Also, it identified no ARMA errors, no dumping parameters, but confirmed the monthly and annual seasonal components.

<br>

One disadvantage of this model is that does not allow to include covariates as control time-series. Additionally, despite that some outcomes did not show any seasonal trends, we decided to incorporate them anyways because of groundedIn backgrounds that add support for their inclusion.

<br>

# Bayesian Structural Time-Series

<br>

```{=html}
<!--- 
Generally, we can write a Bayesian structural model like this:

modelA <- lm(hosp_trauma ~ hosp_circ + year + month + day + weekday + yearday + prevtrh, data15a64[data15a64$tx == 0 & data15a64$txtime == 0,])
modelB <- glm(hosp_trauma ~ hosp_circ + year + month + day + weekday + yearday + prevtrh, family = poisson, data15a64[data15a64$tx == 0 & data15a64$txtime == 0,])
modelD <- hosp_trauma ~ offset(log(offset)) + year + as.Date(date) + month + day + weekday + yearday + prevtrh + difftrh + cons_trauma,
modelC <- glm(hosp_trauma ~ hosp_circ + year + month + day + weekday + yearday + prevtrh, family = quasipoisson, data15a64[data15a64$tx == 0 & data15a64$txtime == 0,])

attr(data15a64_wk$prevtrc, "label") <- "Dependent Variable of Choice Prior Day Value of the Total Trauma Consultations"
attr(data15a64_wk$prevrec, "label") <-  "Dependent Variable of Choice Prior Day Value of the Total Respiratory Consultations"
attr(data15a64_wk$prevtrh, "label") <- "Dependent Variable of Choice Prior Day Value of the Total Trauma Hospitalizations"
attr(data15a64_wk$prevreh, "label") <- "Dependent Variable of Choice Prior Day Value of the Total Respiratory Hospitalizations"

attr(data15a64_wk$difftrc, "label") <- "Difference Between Total Consultations and the Total Trauma Consultations"
attr(data15a64_wk$difftrh, "label") <- "Difference Between Total Hospitalizations and the Total Trauma Hospitalizations"
attr(data15a64_wk$diffrec, "label") <-  "Difference Between Total Consultations and the Total Respiratory Consultations"
attr(data15a64_wk$diffreh, "label") <- "Difference Between Total Hospitalizations and the Total Respiratory Hospitalizations"
attr(data15a64_wk$offset, "label") <- "Sum of the total consultations and total hospitalizations"
--->
```
<br>

From what we could interpret, he split the database in order to obtain some kind of rate in the pre-treatment period (txtime==0, tx==0) by modeling negative-binomial generalized linear regression, and added the year (`year`), date (`date`), month (`month`), day (`day`), day of the week (`weekday`), number of day of the year (`yearday`), Choice Prior Day Value of the Total Trauma Consultations (1st Lagged Value) (`prevtrc`), and the Difference Between Total Consultations and the Total Trauma Consultations (`difftrc`).

<br>

I think what Thomas made was a segmented regression, which fits regression lines or curves to the effects time series before and after the intervention, predicted values for the post-intervention period and then compared them to detect significant changes in slope or level (Cox, 2015).

<br>

In contrast, we used a Bayesian Structural Time Series (BSTS) with Causal Impact Analysis. The model contains a local level component (observation equation linking observed data to a state vector) and a seasonal component (state equation that describes how the state vector evolves over time). Also it can contain other predictor series. The method uses a Markov chain Monte Carlo (MCMC) sampling of the structural time series given the observed data and the priors.

<br>

## Several models contrasted

We decided to contrast several models with different specifications and control variables. We started with a model that did not have a counterfactual.

<br>

### No counterfactual, no control variables

```{r bsts1, echo=T, cache= T, paged.print=TRUE, warning=F}
library(CausalImpact)
#read a csv file that contains the time series: the format should be the following:
#first column: dates
#second column: our interest outcome timeseries on which we want to apply the CausalImpact
#other columns: all the X that we want
#path of the file containing the db, by default you can use the example csv

#PARÁMETROS INGRESADOS A LOS MODELOS POR THOMAS

#EN este modelo, Thomas utilizó un offset(log("Sum of the total consultations and total hospitalizations")), probablemente para corregir por el número de eventos 
#En NB con log-link tiende a utilizarse cuando se asume que la cantidad de consultas u hospitalizaciones depende directamente de la cantidad de consultas y hospitalizaciones, al formar parte de él.
#Podría ser que esté interesado en la tasa

#PARÁMETROS INGRESADOS A LOS MODELOS POR THOMAS
#cons_trauma ~ offset(log(offset)) + year + as.Date(date) + month + day + weekday + yearday + prevtrc + difftrc + hosp_trauma,
#hosp_trauma ~ offset(log(offset)) + year + as.Date(date) + month + day + weekday + yearday + prevtrh + difftrh + cons_trauma,
#data15a64[data15a64$tx == 0 & data15a64$txtime == 0, ]
#glm(hosp_trauma ~ hosp_circ + year + month + day + weekday + yearday + prevtrh, family = quasipoisson(), data15a64[data15a64$tx == 0 & data15a64$txtime == 1,])
#glm(hosp_trauma ~ hosp_circ + year + month + day + weekday + yearday + prevtrh, family = poisson(), data15a64[data15a64$tx == 0 & data15a64$txtime == 1,])
#lm(hosp_trauma ~ hosp_circ + year + month + day + weekday + yearday + prevtrh, data15a64[data15a64$tx == 0 & data15a64$txtime == 1,])

data15a64_rn2<-
data15a64_rn%>%
  dplyr::mutate(log_offset=log(offset))%>%
  dplyr::mutate(log_hosp_trauma=log(hosp_trauma))%>%
  dplyr::mutate(log_hosp_circ=log(hosp_circ))%>%
  dplyr::mutate(log_hosp_resp=log(hosp_resp))%>%
  dplyr::mutate(log_difftrc=log(difftrc))%>%
  data.frame()

#rn_summ_data15a64

#CREATE THE DB --> SUPER IMPORTANT TO ADD THE OUTPUT OF INTEREST AS THE FIRST COLUMN here, plus to select all the other time series we want:
#in this case we take all the columns from 2 to 10
df <- zoo(data15a64_rn2[c("hosp_trauma","hosp_circ","hosp_resp","difftrc")], as.Date(data15a64_rn2$date)) #prevtrh had a missing value due to the absence of previous value
#set the period before intervention
pre.period <- as.Date(c("2015-01-01", "2019-10-17"))
#set the period after intervention
post.period <- as.Date(c("2019-10-18", "2019-12-31"))
#compute CausalImpact, look at the documentation to change niter and nseasons
set.seed(2125)
impact <- CausalImpact(df, pre.period,post.period, model.args=list(niter=clus_iter, nseasons=7))
plot(impact, "original") 

plot(impact)
##summary(impact) 
##summary(impact,"report")
```

<br>

### With counterfactuals

We decided to exclude the post-treatment period data to specify our model, to mimic the fact that this data should be unobserved after the intervention. We introduced yearly and monthly seasonal components into the time-series structure. We started with a Random Walk. This local level model assumes the trend is a random walk (do not assume an observable pattern or trend).

<br>

```{r bsts2, echo=T, cache= T, paged.print=TRUE, warning=F}
##data%>% dplyr::filter(yearday>0)%>% group_by(year)%>% summarise(min(as.Date(date)),max(as.Date(date)),n())

#I will explore their cumulative 1 step forward prediction error for the time series between the start to the time of intervention
# Post Intervention Period is filled with NA
# Remove outcomes from the post-period. The BSTS model should be ignorant of the values we intend to predict

#_#_#_#_#_#_#_#_#_#_#_#_#_
#MADE WITH VECTORS: https://rpubs.com/irJERAD/Causal-Impact
#_#_#_#_#_#_#_#_#_#_#_#_#_#_
#
data15a64_rn_causal <- data15a64_rn %>%
    dplyr::mutate(hosp_trauma = replace(hosp_trauma, did >= 1, NA)) %>% 
    dplyr::mutate(log_hosp_trauma=log(hosp_trauma))%>%
    dplyr::mutate(log_hosp_circ=log(hosp_circ))%>%
    dplyr::mutate(log_hosp_resp=log(hosp_resp))%>%
    dplyr::mutate(log_difftrc=log(difftrc))


post_period_response <- as.numeric(unlist(data15a64_rn[which(data15a64_rn$did==1),"hosp_trauma"]))

y<- data15a64_rn_causal$hosp_trauma
x1 <-data15a64_rn_causal$hosp_circ

#y<- data15a64_rn_causal$log_hosp_trauma
#x1 <-data15a64_rn_causal$log_hosp_circ
#x2 <-data15a64_rn_causal$log_hosp_resp
#x3 <-data15a64_rn_causal$log_difftrc
#x4 <-data15a64_rn_causal$log_offset

# Model 1
ss <- list()
# Local trend, weekly-seasonal #https://qastack.mx/stats/209426/predictions-from-bsts-model-in-r-are-failing-completely - PUSE UN GENERALIZED LOCAL TREND
ss <- AddLocalLevel(ss, data15a64_rn_causal$hosp_trauma) #
# Add weekly seasonal
ss <- AddSeasonal(ss, data15a64_rn_causal$hosp_trauma, nseasons=5, season.duration = 52) #weeks OJO, ESTOS NO SON WEEKS VERDADEROS. PORQUE TENGO MAS DE EUN AÑO
ss <- AddSeasonal(ss, data15a64_rn_causal$hosp_trauma, nseasons = 12, season.duration = 4) #months
# For example, to add a day-of-week component to data with daily granularity, use model.args = list(nseasons = 7, season.duration = 1). To add a day-of-week component to data with hourly granularity, set model.args = list(nseasons = 7, season.duration = 24).
model1 <- bsts(data15a64_rn_causal$hosp_trauma, 
               state.specification = ss, #A list with elements created by AddLocalLinearTrend, AddSeasonal, and similar functions for adding components of state. See the help page for state.specification.
               #family ="poisson", #A Bayesian Analysis of Time-Series Event Count Data
               niter = clus_iter, 
               #burn = 200, #http://finzi.psych.upenn.edu/library/bsts/html/SuggestBurn.html Suggest the size of an MCMC burn in sample as a proportion of the total run.
               seed= 2125)
plot(model1, main = "Model 1")
plot(model1, "components")

impact2 <- CausalImpact(bsts.model = model1,
                       post.period.response = post_period_response)
plot(impact2, "original") 

burn1 <- SuggestBurn(0.1, model1)

plot(impact2)
#summary(impact2) 
##summary(impact2,"report")
#msts(data15a64_rn$hosp_trauma, seasonal.periods=c(7,365.25))
```

```{r bsts3, echo=T, cache= T, paged.print=TRUE, warning=F,eval=T}
#The response variable (i.e., the first column in data) may contain missing values (NA), but covariates (all other columns in data) may not. If one of your covariates contains missing values, consider imputing (i.e., estimating) the missing values; if this is not feasible, leave the regressor out.

# Model 2
ss2 <- list()
# Local trend, weekly-seasonal #https://qastack.mx/stats/209426/predictions-from-bsts-model-in-r-are-failing-completely - PUSE UN GENERALIZED LOCAL TREND
ss2 <- AddLocalLevel(ss2, data15a64_rn_causal$hosp_trauma) #
# Add weekly seasonal
ss2 <- AddSeasonal(ss2, data15a64_rn_causal$hosp_trauma, nseasons=5, season.duration = 52) #weeks OJO, ESTOS NO SON WEEKS VERDADEROS. PORQUE TENGO MAS DE EUN AÑO
ss2 <- AddSeasonal(ss2, data15a64_rn_causal$hosp_trauma, nseasons = 12, season.duration =4) #years
#ss2 <- AddAutoAr(ss2, y = data15a64_rn_causal$log_hosp_trauma, lags = 1) #NO PUEDO AREGAR AR1 CON POISSON
# For example, to add a day-of-week component to data with daily granularity, use model.args = list(nseasons = 7, season.duration = 1). To add a day-of-week component to data with hourly granularity, set model.args = list(nseasons = 7, season.duration = 24).
model2 <- bsts(data15a64_rn_causal$hosp_trauma ~ x1, 
               state.specification = ss2, #A list with elements created by AddLocalLinearTrend, AddSeasonal, and similar functions for adding components of state. See the help page for state.specification.
               #family ="poisson", #A Bayesian Analysis of Time-Series Event Count Data
               niter = clus_iter, 
              # burn = 500, #http://finzi.psych.upenn.edu/library/bsts/html/SuggestBurn.html Suggest the size of an MCMC burn in sample as a proportion of the total run.
               seed= 2125)
plot(model2, main = "Model 2")
plot(model2, "components")

impact3 <- CausalImpact(bsts.model = model2,
                       post.period.response = post_period_response)
plot(impact3, "original") 

burn2 <- SuggestBurn(0.1, model2)

plot(impact3)
#summary(impact3) 
##summary(impact3,"report")
#dynamic.regression Whether to include time-varying regression coefficients. In combination with a time-varying local trend or even a time-varying local level, this often leads to overspecification, in which case a static regression is safer. Defaults to FALSE.
```

<br>

### Local Linear Trend

A model with too many components can sometimes offer too much flexibility, providing unrealistically widening forecasts. This is why the default model does not include a local linear trend component [(see this link)](https://stats.stackexchange.com/questions/266129/causalimpact-model-in-the-paper-and-default-in-the-package/267170#267170). But in the next two models we assumed a local structure of the latent state variable (`AddLocalLinearTrend`), instead of a random process.

```{=html}
<!--- 
This model differs from the local linear trend model in that the latter assumes the slope  follows a random walk. A stationary AR(1) process is less variable than a random walk when making projections far into the future, so this model often gives more reasonable uncertainty estimates when making long term forecasts.
 
 This could be due to the fact that the drift component in the semi local linear trend comprised of more variables (D, ρ) that allowed for more extreme stochasticity. This allowed for the semi-local linear trend models to capture certain high spike points such as in Jan 2018 that were not captured by the trends in the local linear trend models.
--->
```
<br>

```{r bsts2b, echo=T, cache= T, paged.print=TRUE, warning=F}
# Model 1
ssb <- list()
# Local trend, weekly-seasonal #https://qastack.mx/stats/209426/predictions-from-bsts-model-in-r-are-failing-completely - PUSE UN GENERALIZED LOCAL TREND
ssb <- AddLocalLinearTrend(ssb, data15a64_rn_causal$hosp_trauma) #AddSemilocalLinearTrend #AddLocalLevel
# Add weekly seasonal
ssb <- AddSeasonal(ssb, data15a64_rn_causal$hosp_trauma, nseasons=5, season.duration = 52) #weeks OJO, ESTOS NO SON WEEKS VERDADEROS. PORQUE TENGO MAS DE EUN AÑO
ssb <- AddSeasonal(ssb, data15a64_rn_causal$hosp_trauma, nseasons = 12, season.duration =4) #years

# For example, to add a day-of-week component to data with daily granularity, use model.args = list(nseasons = 7, season.duration = 1). To add a day-of-week component to data with hourly granularity, set model.args = list(nseasons = 7, season.duration = 24).
model1b <- bsts(data15a64_rn_causal$hosp_trauma, 
               state.specification = ssb, #A list with elements created by AddLocalLinearTrend, AddSeasonal, and similar functions for adding components of state. See the help page for state.specification.
               #family ="poisson", #A Bayesian Analysis of Time-Series Event Count Data. NO SE PUEDE OCUPAR
               niter = clus_iter, 
               #burn = 200, #http://finzi.psych.upenn.edu/library/bsts/html/SuggestBurn.html Suggest the size of an MCMC burn in sample as a proportion of the total run.
               seed= 2125)
#,
#               dynamic.regression=T)
plot(model1b, main = "Model 1")
plot(model1b, "components")

impact2b <- CausalImpact(bsts.model = model1b,
                       post.period.response = post_period_response)
plot(impact2b, "original") 

burn1b <- SuggestBurn(0.1, model1b)

plot(impact2b)
#summary(impact2b) 
##summary(impact2b,"report")
#msts(data15a64_rn$hosp_trauma, seasonal.periods=c(7,365.25))
```

```{r bsts3b, echo=T, cache= T, paged.print=TRUE, warning=F,eval=T}
#The response variable (i.e., the first column in data) may contain missing values (NA), but covariates (all other columns in data) may not. If one of your covariates contains missing values, consider imputing (i.e., estimating) the missing values; if this is not feasible, leave the regressor out.

# Model 2
ss2b <- list()
# Local trend, weekly-seasonal #https://qastack.mx/stats/209426/predictions-from-bsts-model-in-r-are-failing-completely - PUSE UN GENERALIZED LOCAL TREND
ss2b <- AddLocalLinearTrend(ss2b, data15a64_rn_causal$hosp_trauma) #
# Add weekly seasonal
ss2b <- AddSeasonal(ss2b, data15a64_rn_causal$hosp_trauma, nseasons=5, season.duration = 52) #weeks OJO, ESTOS NO SON WEEKS VERDADEROS. PORQUE TENGO MAS DE EUN AÑO
ss2b <- AddSeasonal(ss2b, data15a64_rn_causal$hosp_trauma, nseasons = 12, season.duration =4) #years
#ss2 <- AddAutoAr(ss2, y = data15a64_rn_causal$log_hosp_trauma, lags = 1) #NO PUEDO AREGAR AR1 CON POISSON
# For example, to add a day-of-week component to data with daily granularity, use model.args = list(nseasons = 7, season.duration = 1). To add a day-of-week component to data with hourly granularity, set model.args = list(nseasons = 7, season.duration = 24).
model2b <- bsts(data15a64_rn_causal$hosp_trauma ~ x1,
               state.specification = ss2b, #A list with elements created by AddLocalLinearTrend, AddSeasonal, and similar functions for adding components of state. See the help page for state.specification.
              # family ="poisson", #A Bayesian Analysis of Time-Series Event Count Data
               niter = clus_iter, 
              # burn = 500, #http://finzi.psych.upenn.edu/library/bsts/html/SuggestBurn.html Suggest the size of an MCMC burn in sample as a proportion of the total run.
               seed= 2125)
#,
#               dynamic.regression=T)
plot(model2b, main = "Model 2")
plot(model2b, "components")

impact3b <- CausalImpact(bsts.model = model2b,
                       post.period.response = post_period_response)
plot(impact3b, "original") 

burn2b <- SuggestBurn(0.1, model2b)

plot(impact3b)
#summary(impact3b) 
##summary(impact3b,"report")
#dynamic.regression Whether to include time-varying regression coefficients. In combination with a time-varying local trend or even a time-varying local level, this often leads to overspecification, in which case a static regression is safer. Defaults to FALSE.
#
#Prior standard deviation of the Gaussian random walk of the local level. Expressed in terms of data standard deviations. Defaults to 0.01, a typical choice for well-behaved and stable datasets with low residual volatility after regressing out known predictors (e.g., web searches or sales in high quantities). When in doubt, a safer option is to use 0.1, as validated on synthetic data, although this may sometimes give rise to unrealistically wide prediction intervals.
```

<br>

### Local Linear Trend & AR

<br>

We also introduced an a sparse AR(1) process to the state distribution.

<br>

```{r bsts2b_ar, echo=T, cache= T, paged.print=TRUE, warning=F}
# Model 1
ssb <- list()
# Local trend, weekly-seasonal #https://qastack.mx/stats/209426/predictions-from-bsts-model-in-r-are-failing-completely - PUSE UN GENERALIZED LOCAL TREND
ssb <- AddLocalLinearTrend(ssb, data15a64_rn_causal$hosp_trauma) #AddSemilocalLinearTrend #AddLocalLevel
ssb <- AddAutoAr(ssb,data15a64_rn_causal$hosp_trauma)

# Add weekly seasonal
ssb <- AddSeasonal(ssb, data15a64_rn_causal$hosp_trauma, nseasons=5, season.duration = 52) #weeks OJO, ESTOS NO SON WEEKS VERDADEROS. PORQUE TENGO MAS DE EUN AÑO
ssb <- AddSeasonal(ssb, data15a64_rn_causal$hosp_trauma, nseasons = 12, season.duration =4) #years

# For example, to add a day-of-week component to data with daily granularity, use model.args = list(nseasons = 7, season.duration = 1). To add a day-of-week component to data with hourly granularity, set model.args = list(nseasons = 7, season.duration = 24).
model1bar <- bsts(data15a64_rn_causal$hosp_trauma, 
               state.specification = ssb, #A list with elements created by AddLocalLinearTrend, AddSeasonal, and similar functions for adding components of state. See the help page for state.specification.
               #family ="poisson", #A Bayesian Analysis of Time-Series Event Count Data. NO SE PUEDE OCUPAR
               niter = clus_iter, 
               #burn = 200, #http://finzi.psych.upenn.edu/library/bsts/html/SuggestBurn.html Suggest the size of an MCMC burn in sample as a proportion of the total run.
               seed= 2125)
#,
#               dynamic.regression=T)
plot(model1bar, main = "Model 1")
plot(model1bar, "components")

impact2bar <- CausalImpact(bsts.model = model1bar,
                       post.period.response = post_period_response)
plot(impact2bar, "original") 

burn1bar <- SuggestBurn(0.1, model1bar)

plot(impact2bar)
#summary(impact2b) 
##summary(impact2b,"report")
#msts(data15a64_rn$hosp_trauma, seasonal.periods=c(7,365.25))
```

```{r bsts3b_ar, echo=T, cache= T, paged.print=TRUE, warning=F,eval=T}
#The response variable (i.e., the first column in data) may contain missing values (NA), but covariates (all other columns in data) may not. If one of your covariates contains missing values, consider imputing (i.e., estimating) the missing values; if this is not feasible, leave the regressor out.

# Model 2
ss2b <- list()
# Local trend, weekly-seasonal #https://qastack.mx/stats/209426/predictions-from-bsts-model-in-r-are-failing-completely - PUSE UN GENERALIZED LOCAL TREND
ss2b <- AddLocalLinearTrend(ss2b, data15a64_rn_causal$hosp_trauma) #
ss2b <- AddAutoAr(ss2b,data15a64_rn_causal$hosp_trauma)

# Add weekly seasonal
ss2b <- AddSeasonal(ss2b, data15a64_rn_causal$hosp_trauma, nseasons=5, season.duration = 52) #weeks OJO, ESTOS NO SON WEEKS VERDADEROS. PORQUE TENGO MAS DE EUN AÑO
ss2b <- AddSeasonal(ss2b, data15a64_rn_causal$hosp_trauma, nseasons = 12, season.duration =4) #years
#ss2 <- AddAutoAr(ss2, y = data15a64_rn_causal$log_hosp_trauma, lags = 1) #NO PUEDO AREGAR AR1 CON POISSON
# For example, to add a day-of-week component to data with daily granularity, use model.args = list(nseasons = 7, season.duration = 1). To add a day-of-week component to data with hourly granularity, set model.args = list(nseasons = 7, season.duration = 24).
model2bar <- bsts(data15a64_rn_causal$hosp_trauma ~ x1,
               state.specification = ss2b, #A list with elements created by AddLocalLinearTrend, AddSeasonal, and similar functions for adding components of state. See the help page for state.specification.
              # family ="poisson", #A Bayesian Analysis of Time-Series Event Count Data
               niter = clus_iter, 
              # burn = 500, #http://finzi.psych.upenn.edu/library/bsts/html/SuggestBurn.html Suggest the size of an MCMC burn in sample as a proportion of the total run.
               seed= 2125)
#,
#               dynamic.regression=T)
plot(model2bar, main = "Model 2")
plot(model2bar, "components")

impact3bar <- CausalImpact(bsts.model = model2b,
                       post.period.response = post_period_response)
plot(impact3bar, "original") 

burn2bar <- SuggestBurn(0.1, model2bar)

plot(impact3bar)
#summary(impact3b) 
##summary(impact3b,"report")
#dynamic.regression Whether to include time-varying regression coefficients. In combination with a time-varying local trend or even a time-varying local level, this often leads to overspecification, in which case a static regression is safer. Defaults to FALSE.
#
#Prior standard deviation of the Gaussian random walk of the local level. Expressed in terms of data standard deviations. Defaults to 0.01, a typical choice for well-behaved and stable datasets with low residual volatility after regressing out known predictors (e.g., web searches or sales in high quantities). When in doubt, a safer option is to use 0.1, as validated on synthetic data, although this may sometimes give rise to unrealistically wide prediction intervals.
```

<br>

### Prior Local Level SD=.1 (more restrictive)

We used the term `prior.level.sd` to .1 instead of the default .01, which had been a typical choice for well-behaved and stable datasets with low residual volatility. We decided to distinguish between models that assumed a Random Walk structure, to local linear trends.

<br>

```{r bsts2c, echo=T, cache= T, paged.print=TRUE, warning=F}
# Model 1
ssc <- list()
# Local trend, weekly-seasonal #https://qastack.mx/stats/209426/predictions-from-bsts-model-in-r-are-failing-completely - PUSE UN GENERALIZED LOCAL TREND
ssc <- AddLocalLevel(ssc, data15a64_rn_causal$hosp_trauma) #AddSemilocalLinearTrend #AddLocalLevel
# Add weekly seasonal
ssc <- AddSeasonal(ssc, data15a64_rn_causal$hosp_trauma, nseasons=5, season.duration = 52) #weeks OJO, ESTOS NO SON WEEKS VERDADEROS. PORQUE TENGO MAS DE EUN AÑO
ssc <- AddSeasonal(ssc, data15a64_rn_causal$hosp_trauma, nseasons = 12, season.duration =4) #years
# For example, to add a day-of-week component to data with daily granularity, use model.args = list(nseasons = 7, season.duration = 1). To add a day-of-week component to data with hourly granularity, set model.args = list(nseasons = 7, season.duration = 24).
model1c <- bsts(data15a64_rn_causal$hosp_trauma, 
               state.specification = ssc, #A list with elements created by AddLocalLinearTrend, AddSeasonal, and similar functions for adding components of state. See the help page for state.specification.
               #family ="poisson", #A Bayesian Analysis of Time-Series Event Count Data. NO SE PUEDE OCUPAR
               niter = clus_iter, 
               #burn = 200, #http://finzi.psych.upenn.edu/library/bsts/html/SuggestBurn.html Suggest the size of an MCMC burn in sample as a proportion of the total run.
               seed= 2125)
#,
#               dynamic.regression=T)
plot(model1c, main = "Model 1")
plot(model1c, "components")

impact2c <- CausalImpact(bsts.model = model1c,model.args = list(prior.level.sd=.1),
                       post.period.response = post_period_response)
plot(impact2c, "original") 

burn1c <- SuggestBurn(0.1, model1c)

plot(impact2c)
#summary(impact2c) 
##summary(impact2c,"report")
#msts(data15a64_rn$hosp_trauma, seasonal.periods=c(7,365.25))
```

```{r bsts3c, echo=T, cache= T, paged.print=TRUE, warning=F,eval=T}
#The response variable (i.e., the first column in data) may contain missing values (NA), but covariates (all other columns in data) may not. If one of your covariates contains missing values, consider imputing (i.e., estimating) the missing values; if this is not feasible, leave the regressor out.

# Model 2
ss2c <- list()
# Local trend, weekly-seasonal #https://qastack.mx/stats/209426/predictions-from-bsts-model-in-r-are-failing-completely - PUSE UN GENERALIZED LOCAL TREND
ss2c <- AddLocalLevel(ss2c, data15a64_rn_causal$hosp_trauma) #
# Add weekly seasonal
ss2c <- AddSeasonal(ss2c, data15a64_rn_causal$hosp_trauma, nseasons=5, season.duration = 52) #weeks OJO, ESTOS NO SON WEEKS VERDADEROS. PORQUE TENGO MAS DE EUN AÑO
ss2c <- AddSeasonal(ss2c, data15a64_rn_causal$hosp_trauma, nseasons = 12, season.duration =4) #years
#ss2 <- AddAutoAr(ss2, y = data15a64_rn_causal$log_hosp_trauma, lags = 1) #NO PUEDO AREGAR AR1 CON POISSON
# For example, to add a day-of-week component to data with daily granularity, use model.args = list(nseasons = 7, season.duration = 1). To add a day-of-week component to data with hourly granularity, set model.args = list(nseasons = 7, season.duration = 24).
model2c <- bsts(data15a64_rn_causal$hosp_trauma ~ x1,
               state.specification = ss2c, #A list with elements created by AddLocalLinearTrend, AddSeasonal, and similar functions for adding components of state. See the help page for state.specification.
              # family ="poisson", #A Bayesian Analysis of Time-Series Event Count Data
               niter = clus_iter, 
              # burn = 500, #http://finzi.psych.upenn.edu/library/bsts/html/SuggestBurn.html Suggest the size of an MCMC burn in sample as a proportion of the total run.
               seed= 2125)
#,
#               dynamic.regression=T)
plot(model2c, main = "Model 2")
plot(model2c, "components")

impact3c <- CausalImpact(bsts.model = model2c,model.args = list(prior.level.sd=.1),
                       post.period.response = post_period_response)
plot(impact3c, "original") 

burn2c <- SuggestBurn(0.1, model2c)

plot(impact3c)
#summary(impact3c) 
##summary(impact3c,"report")
#dynamic.regression Whether to include time-varying regression coefficients. In combination with a time-varying local trend or even a time-varying local level, this often leads to overspecification, in which case a static regression is safer. Defaults to FALSE.
#
#Prior standard deviation of the Gaussian random walk of the local level. Expressed in terms of data standard deviations. Defaults to 0.01, a typical choice for well-behaved and stable datasets with low residual volatility after regressing out known predictors (e.g., web searches or sales in high quantities). When in doubt, a safer option is to use 0.1, as validated on synthetic data, although this may sometimes give rise to unrealistically wide prediction intervals.
```

#### Prior Local SD=.1 (more restrictive) w/ Local Linear Trends

<br>

We changed the assumption of a random walk model to a local-linear trend to the structure of the series.

<br>

```{r bsts2c3, echo=T, cache= T, paged.print=TRUE, warning=F}
# Model 1
ssc3 <- list()
# Local trend, weekly-seasonal #https://qastack.mx/stats/209426/predictions-from-bsts-model-in-r-are-failing-completely - PUSE UN GENERALIZED LOCAL TREND
ssc3 <- AddLocalLinearTrend(ssc3, data15a64_rn_causal$hosp_trauma) #AddSemilocalLinearTrend #AddLocalLevel
# Add weekly seasonal
ssc3 <- AddSeasonal(ssc3, data15a64_rn_causal$hosp_trauma, nseasons=5, season.duration = 52) #weeks OJO, ESTOS NO SON WEEKS VERDADEROS. PORQUE TENGO MAS DE EUN AÑO
ssc3 <- AddSeasonal(ssc3, data15a64_rn_causal$hosp_trauma, nseasons = 12, season.duration =4) #years
# For example, to add a day-of-week component to data with daily granularity, use model.args = list(nseasons = 7, season.duration = 1). To add a day-of-week component to data with hourly granularity, set model.args = list(nseasons = 7, season.duration = 24).
model1c3 <- bsts(data15a64_rn_causal$hosp_trauma, 
               state.specification = ssc3, #A list with elements created by AddLocalLinearTrend, AddSeasonal, and similar functions for adding components of state. See the help page for state.specification.
               #family ="poisson", #A Bayesian Analysis of Time-Series Event Count Data. NO SE PUEDE OCUPAR
               niter = clus_iter, 
               #burn = 200, #http://finzi.psych.upenn.edu/library/bsts/html/SuggestBurn.html Suggest the size of an MCMC burn in sample as a proportion of the total run.
               seed= 2125)
#,
#               dynamic.regression=T)
plot(model1c3, main = "Model 1")
plot(model1c3, "components")

impact2c3 <- CausalImpact(bsts.model = model1c3,model.args = list(prior.level.sd=.1),
                       post.period.response = post_period_response)
plot(impact2c3, "original") 

burn1c3 <- SuggestBurn(0.1, model1c3)

plot(impact2c3)
#summary(impact2c) 
##summary(impact2c,"report")
#msts(data15a64_rn$hosp_trauma, seasonal.periods=c(7,365.25))
```

```{r bsts3c3, echo=T, cache= T, paged.print=TRUE, warning=F,eval=T}
#The response variable (i.e., the first column in data) may contain missing values (NA), but covariates (all other columns in data) may not. If one of your covariates contains missing values, consider imputing (i.e., estimating) the missing values; if this is not feasible, leave the regressor out.

# Model 2
ss2c3 <- list()
# Local trend, weekly-seasonal #https://qastack.mx/stats/209426/predictions-from-bsts-model-in-r-are-failing-completely - PUSE UN GENERALIZED LOCAL TREND
ss2c3 <- AddLocalLinearTrend(ss2c3, data15a64_rn_causal$hosp_trauma) #
# Add weekly seasonal
ss2c3 <- AddSeasonal(ss2c3, data15a64_rn_causal$hosp_trauma, nseasons=5, season.duration = 52) #weeks OJO, ESTOS NO SON WEEKS VERDADEROS. PORQUE TENGO MAS DE EUN AÑO
ss2c3 <- AddSeasonal(ss2c3, data15a64_rn_causal$hosp_trauma, nseasons = 12, season.duration =4) #years
#ss2 <- AddAutoAr(ss2, y = data15a64_rn_causal$log_hosp_trauma, lags = 1) #NO PUEDO AREGAR AR1 CON POISSON
# For example, to add a day-of-week component to data with daily granularity, use model.args = list(nseasons = 7, season.duration = 1). To add a day-of-week component to data with hourly granularity, set model.args = list(nseasons = 7, season.duration = 24).
model2c3 <- bsts(data15a64_rn_causal$hosp_trauma ~ x1,
               state.specification = ss2c3, #A list with elements created by AddLocalLinearTrend, AddSeasonal, and similar functions for adding components of state. See the help page for state.specification.
              # family ="poisson", #A Bayesian Analysis of Time-Series Event Count Data
               niter = clus_iter, 
              # burn = 500, #http://finzi.psych.upenn.edu/library/bsts/html/SuggestBurn.html Suggest the size of an MCMC burn in sample as a proportion of the total run.
               seed= 2125)
#,
#               dynamic.regression=T)
plot(model2c3, main = "Model 2")
plot(model2c3, "components")

impact3c3 <- CausalImpact(bsts.model = model2c3, model.args = list(prior.level.sd=.1),
                       post.period.response = post_period_response)
plot(impact3c3, "original") 

burn2c3 <- SuggestBurn(0.1, model2c3)

plot(impact3c3)
#summary(impact3c) 
##summary(impact3c,"report")
#dynamic.regression Whether to include time-varying regression coefficients. In combination with a time-varying local trend or even a time-varying local level, this often leads to overspecification, in which case a static regression is safer. Defaults to FALSE.
#
#Prior standard deviation of the Gaussian random walk of the local level. Expressed in terms of data standard deviations. Defaults to 0.01, a typical choice for well-behaved and stable datasets with low residual volatility after regressing out known predictors (e.g., web searches or sales in high quantities). When in doubt, a safer option is to use 0.1, as validated on synthetic data, although this may sometimes give rise to unrealistically wide prediction intervals.
```

#### Prior Local SD=.1 (more restrictive) w/ Local Linear Trends & AR

<br>

We also introduced an a sparse AR(1) process to the state distribution.

<br>

```{r bsts2c3_ar, echo=T, cache= T, paged.print=TRUE, warning=F}
# Model 1
ssc3 <- list()
# Local trend, weekly-seasonal #https://qastack.mx/stats/209426/predictions-from-bsts-model-in-r-are-failing-completely - PUSE UN GENERALIZED LOCAL TREND
ssc3 <- AddLocalLinearTrend(ssc3, data15a64_rn_causal$hosp_trauma) #AddSemilocalLinearTrend #AddLocalLevel
ssc3 <- AddAutoAr(ssc3,data15a64_rn_causal$hosp_trauma)
# Add weekly seasonal
ssc3 <- AddSeasonal(ssc3, data15a64_rn_causal$hosp_trauma, nseasons=5, season.duration = 52) #weeks OJO, ESTOS NO SON WEEKS VERDADEROS. PORQUE TENGO MAS DE EUN AÑO
ssc3 <- AddSeasonal(ssc3, data15a64_rn_causal$hosp_trauma, nseasons = 12, season.duration =4) #years
# For example, to add a day-of-week component to data with daily granularity, use model.args = list(nseasons = 7, season.duration = 1). To add a day-of-week component to data with hourly granularity, set model.args = list(nseasons = 7, season.duration = 24).
model1c3ar <- bsts(data15a64_rn_causal$hosp_trauma, 
               state.specification = ssc3, #A list with elements created by AddLocalLinearTrend, AddSeasonal, and similar functions for adding components of state. See the help page for state.specification.
               #family ="poisson", #A Bayesian Analysis of Time-Series Event Count Data. NO SE PUEDE OCUPAR
               niter = clus_iter, 
               #burn = 200, #http://finzi.psych.upenn.edu/library/bsts/html/SuggestBurn.html Suggest the size of an MCMC burn in sample as a proportion of the total run.
               seed= 2125)
#,
#               dynamic.regression=T)
plot(model1c3ar, main = "Model 1")
plot(model1c3ar, "components")

impact2c3ar <- CausalImpact(bsts.model = model1c3ar,model.args = list(prior.level.sd=.1),
                       post.period.response = post_period_response)
plot(impact2c3ar, "original") 

burn1c3ar <- SuggestBurn(0.1, model1c3ar)

plot(impact2c3ar)
#summary(impact2c) 
##summary(impact2c,"report")
#msts(data15a64_rn$hosp_trauma, seasonal.periods=c(7,365.25))
```

```{r bsts3c3_ar, echo=T, cache= T, paged.print=TRUE, warning=F,eval=T}
#The response variable (i.e., the first column in data) may contain missing values (NA), but covariates (all other columns in data) may not. If one of your covariates contains missing values, consider imputing (i.e., estimating) the missing values; if this is not feasible, leave the regressor out.

# Model 2
ss2c3 <- list()
# Local trend, weekly-seasonal #https://qastack.mx/stats/209426/predictions-from-bsts-model-in-r-are-failing-completely - PUSE UN GENERALIZED LOCAL TREND
ss2c3 <- AddLocalLinearTrend(ss2c3, data15a64_rn_causal$hosp_trauma) #
ss2c3 <- AddAutoAr(ss2c3,data15a64_rn_causal$hosp_trauma)
# Add weekly seasonal
ss2c3 <- AddSeasonal(ss2c3, data15a64_rn_causal$hosp_trauma, nseasons=5, season.duration = 52) #weeks OJO, ESTOS NO SON WEEKS VERDADEROS. PORQUE TENGO MAS DE EUN AÑO
ss2c3 <- AddSeasonal(ss2c3, data15a64_rn_causal$hosp_trauma, nseasons = 12, season.duration =4) #years
#ss2 <- AddAutoAr(ss2, y = data15a64_rn_causal$log_hosp_trauma, lags = 1) #NO PUEDO AREGAR AR1 CON POISSON
# For example, to add a day-of-week component to data with daily granularity, use model.args = list(nseasons = 7, season.duration = 1). To add a day-of-week component to data with hourly granularity, set model.args = list(nseasons = 7, season.duration = 24).
model2c3ar <- bsts(data15a64_rn_causal$hosp_trauma ~ x1,
               state.specification = ss2c3, #A list with elements created by AddLocalLinearTrend, AddSeasonal, and similar functions for adding components of state. See the help page for state.specification.
              # family ="poisson", #A Bayesian Analysis of Time-Series Event Count Data
               niter = clus_iter, 
              # burn = 500, #http://finzi.psych.upenn.edu/library/bsts/html/SuggestBurn.html Suggest the size of an MCMC burn in sample as a proportion of the total run.
               seed= 2125)
#,
#               dynamic.regression=T)
plot(model2c3ar, main = "Model 2")
plot(model2c3ar, "components")

impact3c3ar <- CausalImpact(bsts.model = model2c3ar, model.args = list(prior.level.sd=.1),
                       post.period.response = post_period_response)
plot(impact3c3ar, "original") 

burn2c3ar <- SuggestBurn(0.1, model2c3ar)

plot(impact3c3ar)
#summary(impact3c) 
##summary(impact3c,"report")
#dynamic.regression Whether to include time-varying regression coefficients. In combination with a time-varying local trend or even a time-varying local level, this often leads to overspecification, in which case a static regression is safer. Defaults to FALSE.
#
#Prior standard deviation of the Gaussian random walk of the local level. Expressed in terms of data standard deviations. Defaults to 0.01, a typical choice for well-behaved and stable datasets with low residual volatility after regressing out known predictors (e.g., web searches or sales in high quantities). When in doubt, a safer option is to use 0.1, as validated on synthetic data, although this may sometimes give rise to unrealistically wide prediction intervals.
```

#### Studentized Distributed Noise (more restrictive)

<br>

The default model assumes a Gaussian noise and a Gaussian random walk. In order to handle outliers, we assumed studentized distributed noise.

<br>

```{r bsts2d, echo=T, cache= T, paged.print=TRUE, warning=F}
# Model 1
ssd <- list()
# Local trend, weekly-seasonal #https://qastack.mx/stats/209426/predictions-from-bsts-model-in-r-are-failing-completely - PUSE UN GENERALIZED LOCAL TREND
ssd <- AddLocalLevel(ssd, data15a64_rn_causal$hosp_trauma) #AddSemilocalLinearTrend #AddLocalLevel
# Add weekly seasonal
ssd <- AddSeasonal(ssd, data15a64_rn_causal$hosp_trauma,nseasons=5, season.duration = 52) #weeks OJO, ESTOS NO SON WEEKS VERDADEROS. PORQUE TENGO MAS DE EUN AÑO
ssd <- AddSeasonal(ssd, data15a64_rn_causal$hosp_trauma, nseasons = 12, season.duration =4) #years
# For example, to add a day-of-week component to data with daily granularity, use model.args = list(nseasons = 7, season.duration = 1). To add a day-of-week component to data with hourly granularity, set model.args = list(nseasons = 7, season.duration = 24).
model1d1 <- bsts(data15a64_rn_causal$hosp_trauma, 
               state.specification = ssd, #A list with elements created by AddLocalLinearTrend, AddSeasonal, and similar functions for adding components of state. See the help page for state.specification.
               family ="student", #A Bayesian Analysis of Time-Series Event Count Data. POISSON NO SE PUEDE OCUPAR
               niter = clus_iter, 
               #burn = 200, #http://finzi.psych.upenn.edu/library/bsts/html/SuggestBurn.html Suggest the size of an MCMC burn in sample as a proportion of the total run.
               seed= 2125)
#,
#               dynamic.regression=T)
plot(model1d1, main = "Model 1")
plot(model1d1, "components")

impact2d1 <- CausalImpact(bsts.model = model1d1,
                       post.period.response = post_period_response)
plot(impact2d1, "original") 

burn1d1 <- SuggestBurn(0.1, model1d1)

plot(impact2d1)
#summary(impact2d1) 
##summary(impact2d1,"report")
#msts(data15a64_rn$hosp_trauma, seasonal.periods=c(7,365.25))
```

```{r bsts3d, echo=T, cache= T, paged.print=TRUE, warning=F,eval=T}
#The response variable (i.e., the first column in data) may contain missing values (NA), but covariates (all other columns in data) may not. If one of your covariates contains missing values, consider imputing (i.e., estimating) the missing values; if this is not feasible, leave the regressor out.

# Model 2
ss2d <- list()
# Local trend, weekly-seasonal #https://qastack.mx/stats/209426/predictions-from-bsts-model-in-r-are-failing-completely - PUSE UN GENERALIZED LOCAL TREND
ss2d <- AddLocalLevel(ss2d, data15a64_rn_causal$hosp_trauma) #
# Add weekly seasonal
ss2d <- AddSeasonal(ss2d, data15a64_rn_causal$hosp_trauma,nseasons=5, season.duration = 52) #weeks OJO, ESTOS NO SON WEEKS VERDADEROS. PORQUE TENGO MAS DE EUN AÑO
ss2d <- AddSeasonal(ss2d, data15a64_rn_causal$hosp_trauma, nseasons = 12, season.duration =4) #years
#ss2 <- AddAutoAr(ss2, y = data15a64_rn_causal$log_hosp_trauma, lags = 1) #NO PUEDO AREGAR AR1 CON POISSON
# For example, to add a day-of-week component to data with daily granularity, use model.args = list(nseasons = 7, season.duration = 1). To add a day-of-week component to data with hourly granularity, set model.args = list(nseasons = 7, season.duration = 24).
model2d1 <- bsts(data15a64_rn_causal$hosp_trauma ~ x1,
               state.specification = ss2d, #A list with elements created by AddLocalLinearTrend, AddSeasonal, and similar functions for adding components of state. See the help page for state.specification.
               family ="student", #A Bayesian Analysis of Time-Series Event Count Data
               niter = clus_iter, 
              # burn = 500, #http://finzi.psych.upenn.edu/library/bsts/html/SuggestBurn.html Suggest the size of an MCMC burn in sample as a proportion of the total run.
               seed= 2125)
#,
#               dynamic.regression=T)
plot(model2d1, main = "Model 2")
plot(model2d1, "components")

impact3d1 <- CausalImpact(bsts.model = model2d1,
                       post.period.response = post_period_response)
plot(impact3d1, "original") 

burn2d1 <- SuggestBurn(0.1, model2d1)

plot(impact3d1)
#summary(impact3d1) 
##summary(impact3d1,"report")
#dynamic.regression Whether to include time-varying regression coefficients. In combination with a time-varying local trend or even a time-varying local level, this often leads to overspecification, in which case a static regression is safer. Defaults to FALSE.
#
#Prior standard deviation of the Gaussian random walk of the local level. Expressed in terms of data standard deviations. Defaults to 0.01, a typical choice for well-behaved and stable datasets with low residual volatility after regressing out known predictors (e.g., web searches or sales in high quantities). When in doubt, a safer option is to use 0.1, as validated on synthetic data, although this may sometimes give rise to unrealistically wide prediction intervals.
```

##### Studentized Distributed Noise w/ Local Linear Trends

<br>

```{r bsts2d22, echo=T, cache= T, paged.print=TRUE, warning=F}
# Model 1
ssd <- list()
# Local trend, weekly-seasonal #https://qastack.mx/stats/209426/predictions-from-bsts-model-in-r-are-failing-completely - PUSE UN GENERALIZED LOCAL TREND
ssd <- AddLocalLinearTrend(ssd, data15a64_rn_causal$hosp_trauma) #AddSemilocalLinearTrend #AddLocalLevel
# Add weekly seasonal
ssd <- AddSeasonal(ssd, data15a64_rn_causal$hosp_trauma,nseasons=5, season.duration = 52) #weeks OJO, ESTOS NO SON WEEKS VERDADEROS. PORQUE TENGO MAS DE EUN AÑO
ssd <- AddSeasonal(ssd, data15a64_rn_causal$hosp_trauma, nseasons = 12, season.duration =4) #years
# For example, to add a day-of-week component to data with daily granularity, use model.args = list(nseasons = 7, season.duration = 1). To add a day-of-week component to data with hourly granularity, set model.args = list(nseasons = 7, season.duration = 24).
model1d2 <- bsts(data15a64_rn_causal$hosp_trauma, 
               state.specification = ssd, #A list with elements created by AddLocalLinearTrend, AddSeasonal, and similar functions for adding components of state. See the help page for state.specification.
               family ="student", #A Bayesian Analysis of Time-Series Event Count Data. POISSON NO SE PUEDE OCUPAR
               niter = clus_iter, 
               #burn = 200, #http://finzi.psych.upenn.edu/library/bsts/html/SuggestBurn.html Suggest the size of an MCMC burn in sample as a proportion of the total run.
               seed= 2125)
#,
#               dynamic.regression=T)
plot(model1d2, main = "Model 1")
plot(model1d2, "components")

impact2d2 <- CausalImpact(bsts.model = model1d2,
                       post.period.response = post_period_response)
plot(impact2d2, "original") 

burn1d2 <- SuggestBurn(0.1, model1d2)

plot(impact2d2)
#summary(impact2d1) 
##summary(impact2d1,"report")
#msts(data15a64_rn$hosp_trauma, seasonal.periods=c(7,365.25))
```

```{r bsts3d22, echo=T, cache= T, paged.print=TRUE, warning=F,eval=T}
#The response variable (i.e., the first column in data) may contain missing values (NA), but covariates (all other columns in data) may not. If one of your covariates contains missing values, consider imputing (i.e., estimating) the missing values; if this is not feasible, leave the regressor out.

# Model 2
ss2d <- list()
# Local trend, weekly-seasonal #https://qastack.mx/stats/209426/predictions-from-bsts-model-in-r-are-failing-completely - PUSE UN GENERALIZED LOCAL TREND
ss2d <- AddLocalLinearTrend(ss2d, data15a64_rn_causal$hosp_trauma) #
# Add weekly seasonal
ss2d <- AddSeasonal(ss2d, data15a64_rn_causal$hosp_trauma,nseasons=5, season.duration = 52) #weeks OJO, ESTOS NO SON WEEKS VERDADEROS. PORQUE TENGO MAS DE EUN AÑO
ss2d <- AddSeasonal(ss2d, data15a64_rn_causal$hosp_trauma, nseasons = 12, season.duration =4) #years
#ss2 <- AddAutoAr(ss2, y = data15a64_rn_causal$log_hosp_trauma, lags = 1) #NO PUEDO AREGAR AR1 CON POISSON
# For example, to add a day-of-week component to data with daily granularity, use model.args = list(nseasons = 7, season.duration = 1). To add a day-of-week component to data with hourly granularity, set model.args = list(nseasons = 7, season.duration = 24).
model2d2 <- bsts(data15a64_rn_causal$hosp_trauma ~ x1,
               state.specification = ss2d, #A list with elements created by AddLocalLinearTrend, AddSeasonal, and similar functions for adding components of state. See the help page for state.specification.
               family ="student", #A Bayesian Analysis of Time-Series Event Count Data
               niter = clus_iter, 
              # burn = 500, #http://finzi.psych.upenn.edu/library/bsts/html/SuggestBurn.html Suggest the size of an MCMC burn in sample as a proportion of the total run.
               seed= 2125)
#,
#               dynamic.regression=T)
plot(model2d2, main = "Model 2")
plot(model2d2, "components")

impact3d2 <- CausalImpact(bsts.model = model2d2,
                       post.period.response = post_period_response)
plot(impact3d2, "original") 

burn2d2 <- SuggestBurn(0.1, model2d2)

plot(impact3d2)
#summary(impact3d1) 
##summary(impact3d1,"report")
#dynamic.regression Whether to include time-varying regression coefficients. In combination with a time-varying local trend or even a time-varying local level, this often leads to overspecification, in which case a static regression is safer. Defaults to FALSE.
#
#Prior standard deviation of the Gaussian random walk of the local level. Expressed in terms of data standard deviations. Defaults to 0.01, a typical choice for well-behaved and stable datasets with low residual volatility after regressing out known predictors (e.g., web searches or sales in high quantities). When in doubt, a safer option is to use 0.1, as validated on synthetic data, although this may sometimes give rise to unrealistically wide prediction intervals.
```

<br>

##### Studentized Distributed Noise w/ Local Linear Trends & AR

<br>

We also introduced an a sparse AR(1) process to the state distribution.

<br>s

```{r bsts2d22_ar, echo=T, cache= T, paged.print=TRUE, warning=F}
# Model 1
ssd <- list()
# Local trend, weekly-seasonal #https://qastack.mx/stats/209426/predictions-from-bsts-model-in-r-are-failing-completely - PUSE UN GENERALIZED LOCAL TREND
ssd <- AddLocalLinearTrend(ssd, data15a64_rn_causal$hosp_trauma) #AddSemilocalLinearTrend #AddLocalLevel
ssd <- AddAutoAr(ssd,data15a64_rn_causal$hosp_trauma)

# Add weekly seasonal
ssd <- AddSeasonal(ssd, data15a64_rn_causal$hosp_trauma,nseasons=5, season.duration = 52) #weeks OJO, ESTOS NO SON WEEKS VERDADEROS. PORQUE TENGO MAS DE EUN AÑO
ssd <- AddSeasonal(ssd, data15a64_rn_causal$hosp_trauma, nseasons = 12, season.duration =4) #years
# For example, to add a day-of-week component to data with daily granularity, use model.args = list(nseasons = 7, season.duration = 1). To add a day-of-week component to data with hourly granularity, set model.args = list(nseasons = 7, season.duration = 24).
model1d2ar <- bsts(data15a64_rn_causal$hosp_trauma, 
               state.specification = ssd, #A list with elements created by AddLocalLinearTrend, AddSeasonal, and similar functions for adding components of state. See the help page for state.specification.
               family ="student", #A Bayesian Analysis of Time-Series Event Count Data. POISSON NO SE PUEDE OCUPAR
               niter = clus_iter, 
               #burn = 200, #http://finzi.psych.upenn.edu/library/bsts/html/SuggestBurn.html Suggest the size of an MCMC burn in sample as a proportion of the total run.
               seed= 2125)
#,
#               dynamic.regression=T)
plot(model1d2ar, main = "Model 1")
plot(model1d2ar, "components")

impact2d2ar <- CausalImpact(bsts.model = model1d2ar,
                       post.period.response = post_period_response)
plot(impact2d2ar, "original") 

burn1d2ar <- SuggestBurn(0.1, model1d2ar)

plot(impact2d2ar)
#summary(impact2d1) 
##summary(impact2d1,"report")
#msts(data15a64_rn$hosp_trauma, seasonal.periods=c(7,365.25))
```

```{r bsts3d22_ar, echo=T, cache= T, paged.print=TRUE, warning=F,eval=T}
#The response variable (i.e., the first column in data) may contain missing values (NA), but covariates (all other columns in data) may not. If one of your covariates contains missing values, consider imputing (i.e., estimating) the missing values; if this is not feasible, leave the regressor out.

# Model 2
ss2d <- list()
# Local trend, weekly-seasonal #https://qastack.mx/stats/209426/predictions-from-bsts-model-in-r-are-failing-completely - PUSE UN GENERALIZED LOCAL TREND
ss2d <- AddLocalLinearTrend(ss2d, data15a64_rn_causal$hosp_trauma)
ss2d <- AddAutoAr(ss2d,data15a64_rn_causal$hosp_trauma)
#
# Add weekly seasonal
ss2d <- AddSeasonal(ss2d, data15a64_rn_causal$hosp_trauma,nseasons=5, season.duration = 52) #weeks OJO, ESTOS NO SON WEEKS VERDADEROS. PORQUE TENGO MAS DE EUN AÑO
ss2d <- AddSeasonal(ss2d, data15a64_rn_causal$hosp_trauma, nseasons = 12, season.duration =4) #years
#ss2 <- AddAutoAr(ss2, y = data15a64_rn_causal$log_hosp_trauma, lags = 1) #NO PUEDO AREGAR AR1 CON POISSON
# For example, to add a day-of-week component to data with daily granularity, use model.args = list(nseasons = 7, season.duration = 1). To add a day-of-week component to data with hourly granularity, set model.args = list(nseasons = 7, season.duration = 24).
model2d2ar <- bsts(data15a64_rn_causal$hosp_trauma ~ x1,
               state.specification = ss2d, #A list with elements created by AddLocalLinearTrend, AddSeasonal, and similar functions for adding components of state. See the help page for state.specification.
               family ="student", #A Bayesian Analysis of Time-Series Event Count Data
               niter = clus_iter, 
              # burn = 500, #http://finzi.psych.upenn.edu/library/bsts/html/SuggestBurn.html Suggest the size of an MCMC burn in sample as a proportion of the total run.
               seed= 2125)
#,
#               dynamic.regression=T)
plot(model2d2ar, main = "Model 2")
plot(model2d2ar, "components")

impact3d2ar <- CausalImpact(bsts.model = model2d2ar,
                       post.period.response = post_period_response)
plot(impact3d2ar, "original") 

burn2d2ar <- SuggestBurn(0.1, model2d2ar)

plot(impact3d2ar)
#summary(impact3d1) 
##summary(impact3d1,"report")
#dynamic.regression Whether to include time-varying regression coefficients. In combination with a time-varying local trend or even a time-varying local level, this often leads to overspecification, in which case a static regression is safer. Defaults to FALSE.
#
#Prior standard deviation of the Gaussian random walk of the local level. Expressed in terms of data standard deviations. Defaults to 0.01, a typical choice for well-behaved and stable datasets with low residual volatility after regressing out known predictors (e.g., web searches or sales in high quantities). When in doubt, a safer option is to use 0.1, as validated on synthetic data, although this may sometimes give rise to unrealistically wide prediction intervals.
```

<br>

#### Studentized Distributed Noise w/ Priors SD=.1 (more restrictive)

<br>

For the following models, we used a prior SD of .1

<br>

```{r bsts2d2, echo=T, cache= T, paged.print=TRUE, warning=F}
# Model 1
ssd <- list()
# Local trend, weekly-seasonal #https://qastack.mx/stats/209426/predictions-from-bsts-model-in-r-are-failing-completely - PUSE UN GENERALIZED LOCAL TREND
ssd <- AddLocalLevel(ssd, data15a64_rn_causal$hosp_trauma) #AddSemilocalLinearTrend #AddLocalLevel AddLocalLinearTrend
# Add weekly seasonal
ssd <- AddSeasonal(ssd, data15a64_rn_causal$hosp_trauma, nseasons=5, season.duration = 52) #weeks OJO, ESTOS NO SON WEEKS VERDADEROS. PORQUE TENGO MAS DE EUN AÑO
ssd <- AddSeasonal(ssd, data15a64_rn_causal$hosp_trauma, nseasons = 12, season.duration =4) #years
# For example, to add a day-of-week component to data with daily granularity, use model.args = list(nseasons = 7, season.duration = 1). To add a day-of-week component to data with hourly granularity, set model.args = list(nseasons = 7, season.duration = 24).
model1d <- bsts(data15a64_rn_causal$hosp_trauma, 
               state.specification = ssd, #A list with elements created by AddLocalLinearTrend, AddSeasonal, and similar functions for adding components of state. See the help page for state.specification.
               family ="student", #A Bayesian Analysis of Time-Series Event Count Data. POISSON NO SE PUEDE OCUPAR
               niter = clus_iter, 
               #burn = 200, #http://finzi.psych.upenn.edu/library/bsts/html/SuggestBurn.html Suggest the size of an MCMC burn in sample as a proportion of the total run.
               seed= 2125)
#,
#               dynamic.regression=T)
plot(model1d, main = "Model 1")
plot(model1d, "components")

impact2d <- CausalImpact(bsts.model = model1d,model.args = list(prior.level.sd=.1),
                       post.period.response = post_period_response)
plot(impact2d, "original") 

burn1d <- SuggestBurn(0.1, model1d)

plot(impact2d)
#summary(impact2d) 
##summary(impact2d,"report")
#msts(data15a64_rn$hosp_trauma, seasonal.periods=c(7,365.25))
```

```{r bsts3d2, echo=T, cache= T, paged.print=TRUE, warning=F,eval=T}
#The response variable (i.e., the first column in data) may contain missing values (NA), but covariates (all other columns in data) may not. If one of your covariates contains missing values, consider imputing (i.e., estimating) the missing values; if this is not feasible, leave the regressor out.

# Model 2
ss2d <- list()
# Local trend, weekly-seasonal #https://qastack.mx/stats/209426/predictions-from-bsts-model-in-r-are-failing-completely - PUSE UN GENERALIZED LOCAL TREND
ss2d <- AddLocalLevel(ss2d, data15a64_rn_causal$hosp_trauma) #
# Add weekly seasonal
ss2d <- AddSeasonal(ss2d, data15a64_rn_causal$hosp_trauma, nseasons=5, season.duration = 52) #weeks OJO, ESTOS NO SON WEEKS VERDADEROS. PORQUE TENGO MAS DE EUN AÑO
ss2d <- AddSeasonal(ss2d, data15a64_rn_causal$hosp_trauma, nseasons = 12, season.duration =4) #years
#ss2 <- AddAutoAr(ss2, y = data15a64_rn_causal$log_hosp_trauma, lags = 1) #NO PUEDO AREGAR AR1 CON POISSON
# For example, to add a day-of-week component to data with daily granularity, use model.args = list(nseasons = 7, season.duration = 1). To add a day-of-week component to data with hourly granularity, set model.args = list(nseasons = 7, season.duration = 24).
model2d <- bsts(data15a64_rn_causal$hosp_trauma ~ x1,
               state.specification = ss2d, #A list with elements created by AddLocalLinearTrend, AddSeasonal, and similar functions for adding components of state. See the help page for state.specification.
               family ="student", #A Bayesian Analysis of Time-Series Event Count Data
               niter = clus_iter, 
              # burn = 500, #http://finzi.psych.upenn.edu/library/bsts/html/SuggestBurn.html Suggest the size of an MCMC burn in sample as a proportion of the total run.
               seed= 2125)
#,
#               dynamic.regression=T)
plot(model2d, main = "Model 2")
plot(model2d, "components")

impact3d <- CausalImpact(bsts.model = model2d,model.args = list(prior.level.sd=.1),
                       post.period.response = post_period_response)
plot(impact3d, "original") 

burn2d <- SuggestBurn(0.1, model2d)

plot(impact3d)
#summary(impact3d) 
##summary(impact3d,"report")
#dynamic.regression Whether to include time-varying regression coefficients. In combination with a time-varying local trend or even a time-varying local level, this often leads to overspecification, in which case a static regression is safer. Defaults to FALSE.
#
#Prior standard deviation of the Gaussian random walk of the local level. Expressed in terms of data standard deviations. Defaults to 0.01, a typical choice for well-behaved and stable datasets with low residual volatility after regressing out known predictors (e.g., web searches or sales in high quantities). When in doubt, a safer option is to use 0.1, as validated on synthetic data, although this may sometimes give rise to unrealistically wide prediction intervals.
```

#### Studentized Distributed Noise w/ Priors SD=.1 (more restrictive) w/ Local Linear Trends

<br>

For the following models, we used a prior SD of .1 and a Local Linear Trend.

<br>

```{r bsts2d3, echo=T, cache= T, paged.print=TRUE, warning=F}
# Model 1
ssd3 <- list()
# Local trend, weekly-seasonal #https://qastack.mx/stats/209426/predictions-from-bsts-model-in-r-are-failing-completely - PUSE UN GENERALIZED LOCAL TREND
ssd3 <- AddLocalLinearTrend(ssd3, data15a64_rn_causal$hosp_trauma) #AddSemilocalLinearTrend #AddLocalLevel AddLocalLinearTrend
#ssd3 <- AddAutoAr(ssd3,data15a64_rn_causal$log_hosp_trauma)
# Add weekly seasonal
ssd3 <- AddSeasonal(ssd3, data15a64_rn_causal$hosp_trauma, nseasons=5, season.duration = 52) #weeks OJO, ESTOS NO SON WEEKS VERDADEROS. PORQUE TENGO MAS DE EUN AÑO
ssd3 <- AddSeasonal(ssd3, data15a64_rn_causal$hosp_trauma, nseasons = 12, season.duration =4) #years
# For example, to add a day-of-week component to data with daily granularity, use model.args = list(nseasons = 7, season.duration = 1). To add a day-of-week component to data with hourly granularity, set model.args = list(nseasons = 7, season.duration = 24).
model1d3 <- bsts(data15a64_rn_causal$hosp_trauma, 
               state.specification = ssd3, #A list with elements created by AddLocalLinearTrend, AddSeasonal, and similar functions for adding components of state. See the help page for state.specification.
               family ="student", #A Bayesian Analysis of Time-Series Event Count Data. POISSON NO SE PUEDE OCUPAR
               niter = clus_iter, 
               #burn = 200, #http://finzi.psych.upenn.edu/library/bsts/html/SuggestBurn.html Suggest the size of an MCMC burn in sample as a proportion of the total run.
               seed= 2125)

burn1d3 <- SuggestBurn(0.1, model1d3)
#,
#               dynamic.regression=T)
plot(model1d3, main = "Model 1")
plot(model1d3, "components")

#pred <- predict(model1d3, horizon = 9, burn =burn1d3)
#updated.pred <- predict(model1d3, horizon = 9, olddata = data15a64_rn_causal$log_hosp_trauma)

impact2d3 <- CausalImpact(bsts.model = model1d3,model.args = list(prior.level.sd=.1),
                       post.period.response = post_period_response)
plot(impact2d3, "original") 

plot(impact2d3)
#summary(impact2d3) 
##summary(impact2d3,"report")
#msts(data15a64_rn$hosp_trauma, seasonal.periods=c(7,365.25))
```

```{r bsts3d3, echo=T, cache= T, paged.print=TRUE, warning=F,eval=T}
#The response variable (i.e., the first column in data) may contain missing values (NA), but covariates (all other columns in data) may not. If one of your covariates contains missing values, consider imputing (i.e., estimating) the missing values; if this is not feasible, leave the regressor out.

# Model 2
ss2d3 <- list()
# Local trend, weekly-seasonal #https://qastack.mx/stats/209426/predictions-from-bsts-model-in-r-are-failing-completely - PUSE UN GENERALIZED LOCAL TREND
ss2d3 <- AddLocalLinearTrend(ss2d3, data15a64_rn_causal$hosp_trauma) #AddSemilocalLinearTrend #AddLocalLevel AddLocalLinearTrend
#ss2d3 <- AddAutoAr(ss2d3,data15a64_rn_causal$hosp_trauma)
# Add weekly seasonal
ss2d3 <- AddSeasonal(ss2d3, data15a64_rn_causal$hosp_trauma, nseasons=5, season.duration = 52) #weeks OJO, ESTOS NO SON WEEKS VERDADEROS. PORQUE TENGO MAS DE EUN AÑO
ss2d3 <- AddSeasonal(ss2d3, data15a64_rn_causal$hosp_trauma, nseasons = 12, season.duration =4) #years
#ss2 <- AddAutoAr(ss2, y = data15a64_rn_causal$log_hosp_trauma, lags = 1) #NO PUEDO AREGAR AR1 CON POISSON
# For example, to add a day-of-week component to data with daily granularity, use model.args = list(nseasons = 7, season.duration = 1). To add a day-of-week component to data with hourly granularity, set model.args = list(nseasons = 7, season.duration = 24).
model2d3 <- bsts(data15a64_rn_causal$hosp_trauma ~ x1,
               state.specification = ss2d3, #A list with elements created by AddLocalLinearTrend, AddSeasonal, and similar functions for adding components of state. See the help page for state.specification.
               family ="student", #A Bayesian Analysis of Time-Series Event Count Data
               niter = clus_iter, 
              # burn = 500, #http://finzi.psych.upenn.edu/library/bsts/html/SuggestBurn.html Suggest the size of an MCMC burn in sample as a proportion of the total run.
               seed= 2125)
#,
#               dynamic.regression=T)
plot(model2d3, main = "Model 2")
plot(model2d3, "components")

impact3d3 <- CausalImpact(bsts.model = model2d3,model.args = list(prior.level.sd=.1),
                       post.period.response = post_period_response)
plot(impact3d3, "original") 

burn2d3 <- SuggestBurn(0.1, model2d3)

plot(impact3d3)
#summary(impact3d3) 
##summary(impact3d3,"report")
#dynamic.regression Whether to include time-varying regression coefficients. In combination with a time-varying local trend or even a time-varying local level, this often leads to overspecification, in which case a static regression is safer. Defaults to FALSE.
#
#Prior standard deviation of the Gaussian random walk of the local level. Expressed in terms of data standard deviations. Defaults to 0.01, a typical choice for well-behaved and stable datasets with low residual volatility after regressing out known predictors (e.g., web searches or sales in high quantities). When in doubt, a safer option is to use 0.1, as validated on synthetic data, although this may sometimes give rise to unrealistically wide prediction intervals.
```

<br>

#### Studentized Distributed Noise w/ Priors SD=.1 (more restrictive) w/ Local Linear Trends & AR

<br>

We also introduced an a sparse AR(1) process to the state distribution.

<br>s

```{r bsts2d3_ar, echo=T, cache= T, paged.print=TRUE, warning=F}
# Model 1
ssd3 <- list()
# Local trend, weekly-seasonal #https://qastack.mx/stats/209426/predictions-from-bsts-model-in-r-are-failing-completely - PUSE UN GENERALIZED LOCAL TREND
ssd3 <- AddLocalLinearTrend(ssd3, data15a64_rn_causal$hosp_trauma) #AddSemilocalLinearTrend #AddLocalLevel AddLocalLinearTrend
ssd3 <- AddAutoAr(ssd3,data15a64_rn_causal$hosp_trauma)
# Add weekly seasonal
ssd3 <- AddSeasonal(ssd3, data15a64_rn_causal$hosp_trauma, nseasons=5, season.duration = 52) #weeks OJO, ESTOS NO SON WEEKS VERDADEROS. PORQUE TENGO MAS DE EUN AÑO
ssd3 <- AddSeasonal(ssd3, data15a64_rn_causal$hosp_trauma, nseasons = 12, season.duration =4) #years
# For example, to add a day-of-week component to data with daily granularity, use model.args = list(nseasons = 7, season.duration = 1). To add a day-of-week component to data with hourly granularity, set model.args = list(nseasons = 7, season.duration = 24).
model1d3ar <- bsts(data15a64_rn_causal$hosp_trauma, 
               state.specification = ssd3, #A list with elements created by AddLocalLinearTrend, AddSeasonal, and similar functions for adding components of state. See the help page for state.specification.
               family ="student", #A Bayesian Analysis of Time-Series Event Count Data. POISSON NO SE PUEDE OCUPAR
               niter = clus_iter, 
               #burn = 200, #http://finzi.psych.upenn.edu/library/bsts/html/SuggestBurn.html Suggest the size of an MCMC burn in sample as a proportion of the total run.
               seed= 2125)

burn1d3ar <- SuggestBurn(0.1, model1d3ar)
#,
#               dynamic.regression=T)
plot(model1d3ar, main = "Model 1")
plot(model1d3ar, "components")

#pred <- predict(model1d3, horizon = 9, burn =burn1d3)
#updated.pred <- predict(model1d3, horizon = 9, olddata = data15a64_rn_causal$log_hosp_trauma)

impact2d3ar <- CausalImpact(bsts.model = model1d3ar,model.args = list(prior.level.sd=.1),
                       post.period.response = post_period_response)
plot(impact2d3ar, "original") 

plot(impact2d3ar)
#summary(impact2d3) 
##summary(impact2d3,"report")
#msts(data15a64_rn$hosp_trauma, seasonal.periods=c(7,365.25))
```

```{r bsts3d3_ar, echo=T, cache= T, paged.print=TRUE, warning=F,eval=T}
#The response variable (i.e., the first column in data) may contain missing values (NA), but covariates (all other columns in data) may not. If one of your covariates contains missing values, consider imputing (i.e., estimating) the missing values; if this is not feasible, leave the regressor out.

# Model 2
ss2d3 <- list()
# Local trend, weekly-seasonal #https://qastack.mx/stats/209426/predictions-from-bsts-model-in-r-are-failing-completely - PUSE UN GENERALIZED LOCAL TREND
ss2d3 <- AddLocalLinearTrend(ss2d3, data15a64_rn_causal$hosp_trauma) #AddSemilocalLinearTrend #AddLocalLevel AddLocalLinearTrend
ss2d3 <- AddAutoAr(ss2d3,data15a64_rn_causal$hosp_trauma)
# Add weekly seasonal
ss2d3 <- AddSeasonal(ss2d3, data15a64_rn_causal$hosp_trauma, nseasons=5, season.duration = 52) #weeks OJO, ESTOS NO SON WEEKS VERDADEROS. PORQUE TENGO MAS DE EUN AÑO
ss2d3 <- AddSeasonal(ss2d3, data15a64_rn_causal$hosp_trauma, nseasons = 12, season.duration =4) #years
#ss2 <- AddAutoAr(ss2, y = data15a64_rn_causal$log_hosp_trauma, lags = 1) #NO PUEDO AREGAR AR1 CON POISSON
# For example, to add a day-of-week component to data with daily granularity, use model.args = list(nseasons = 7, season.duration = 1). To add a day-of-week component to data with hourly granularity, set model.args = list(nseasons = 7, season.duration = 24).
model2d3ar <- bsts(data15a64_rn_causal$hosp_trauma ~ x1,
               state.specification = ss2d3, #A list with elements created by AddLocalLinearTrend, AddSeasonal, and similar functions for adding components of state. See the help page for state.specification.
               family ="student", #A Bayesian Analysis of Time-Series Event Count Data
               niter = clus_iter, 
              # burn = 500, #http://finzi.psych.upenn.edu/library/bsts/html/SuggestBurn.html Suggest the size of an MCMC burn in sample as a proportion of the total run.
               seed= 2125)
#,
#               dynamic.regression=T)
plot(model2d3ar, main = "Model 2")
plot(model2d3ar, "components")

impact3d3ar <- CausalImpact(bsts.model = model2d3ar,model.args = list(prior.level.sd=.1),
                       post.period.response = post_period_response)
plot(impact3d3ar, "original") 

burn2d3 <- SuggestBurn(0.1, model2d3ar)

plot(impact3d3ar)
#summary(impact3d3) 
##summary(impact3d3,"report")
#dynamic.regression Whether to include time-varying regression coefficients. In combination with a time-varying local trend or even a time-varying local level, this often leads to overspecification, in which case a static regression is safer. Defaults to FALSE.
#
#Prior standard deviation of the Gaussian random walk of the local level. Expressed in terms of data standard deviations. Defaults to 0.01, a typical choice for well-behaved and stable datasets with low residual volatility after regressing out known predictors (e.g., web searches or sales in high quantities). When in doubt, a safer option is to use 0.1, as validated on synthetic data, although this may sometimes give rise to unrealistically wide prediction intervals.
```

<br>

------------------------------------------------------------------------

## Comparison Between Models

<br>

We compared the models selected in terms of cumulative absolute error.

<br>

```{r comp_models_prev, echo=T, cache= T, paged.print=TRUE, warning=F,eval=T,fig.show='hide', fig.height=14}
comp_mod<-CompareBstsModels(list("No control variables(a)" = model1,
                                "Control variables(b)" = model2,
                                "a, LocalLinearTrend" = model1b,
                                "b, LocalLinearTrend" = model2b,
                                "a, LocalLinearTrend & AR" = model1bar,
                                "b, LocalLinearTrend & AR" = model2bar,
                                "a, Prior sd=.1" = model1c,
                                "b, Prior sd=.1" = model2c,
                                "a, Prior sd=.1,LocalLinearTrend" = model1c3,
                                "b, Prior sd=.1,LocalLinearTrend" = model2c3,
                                "a, Prior sd=.1,LocalLinearTrend & AR" = model1c3ar,
                                "b, Prior sd=.1,LocalLinearTrend & AR" = model2c3ar,
                                "a, Student Dist, Prior sd=.01" = model1d1, #Student SD=.1 Local
                                "b, Student Dist, Prior sd=.01" = model2d1, #Student SD=.1 Local
                                "a, Student Dist, Prior sd=.01, LocalLinearTrend" = model1d2,
                                "b, Student Dist, Prior sd=.01, LocalLinearTrend" = model2d2,
                                "a, Student Dist, Prior sd=.01, LocalLinearTrend & AR" = model1d2ar,
                                "b, Student Dist, Prior sd=.01, LocalLinearTrend & AR" = model2d2ar,
                                "a, Student Dist, Prior sd=.1" = model1d,
                                "b, Student Dist, Prior sd=.1" = model2d,
                                "a, Student Dist, Prior sd=.1, LocalLinearTrend" = model1d3,
                                "b, Student Dist, Prior sd=.1, LocalLinearTrend" = model2d3,
                                "a, Student Dist, Prior sd=.1, LocalLinearTrend & AR" = model1d3ar,
                                "b, Student Dist, Prior sd=.1, LocalLinearTrend & AR" = model2d3ar
                       ),
                  colors = c("black", "red", "blue","purple","gray","darkslategrey","darkslateblue","darkorange3","forestgreen",
                             "yellow","pink", "brown","cyan"))
```

```{r comp_models, echo=T, cache= T, paged.print=TRUE, warning=F, eval=T, fig.align='center', fig.cap= "Figure 9. Comparison of BSTS models (cum. error)"}
scale2 <- function(x, na.rm = FALSE) (x - mean(x, na.rm = na.rm)) / sd(x, na.rm)

comp_mod_df<-
data.frame(cbind(Models=c("No control variables(a)",
                          "Control variables(b)",
                          "a, LocalLinearTrend",
                          "b, LocalLinearTrend",
                          "a, LocalLinearTrend & AR",
                          "b, LocalLinearTrend & AR",                          
                          "a, Prior sd=.1",
                          "b, Prior sd=.1",
                          "a, Prior sd=.1, LocalLinearTrend",
                          "b, Prior sd=.1, LocalLinearTrend",
                          "a, Prior sd=.1, LocalLinearTrend & AR",
                          "b, Prior sd=.1, LocalLinearTrend & AR",
                          "a, Student Dist, Prior sd=.01",
                          "b, Student Dist, Prior sd=.01",
                          "a, Student Dist, Prior sd=.01, LocalLinearTrend",
                          "b, Student Dist, Prior sd=.01, LocalLinearTrend",
                          "a, Student Dist, Prior sd=.01, LocalLinearTrend & AR",
                          "b, Student Dist, Prior sd=.01, LocalLinearTrend & AR",
                          "a, Student Dist, Prior sd=.1",
                          "b, Student Dist, Prior sd=.1",
                          "a, Student Dist, Prior sd=.1, LocalLinearTrend",
                          "b, Student Dist, Prior sd=.1, LocalLinearTrend",
                          "a, Student Dist, Prior sd=.1, LocalLinearTrend & AR",
                          "b, Student Dist, Prior sd=.1, LocalLinearTrend & AR"),
                 comp_mod)) %>%  #16
  melt(id=1)%>%
  dplyr::rename("Time"="variable") %>% 
  dplyr::rename("Cumulative Absolute Error"="value") %>% 
  dplyr::mutate(Time=as.numeric(sub('V', '', Time))) %>% 
  dplyr::mutate(`Cumulative Absolute Error`=as.numeric(`Cumulative Absolute Error`)) %>% 
  dplyr::mutate(text=paste0("CAFE= ",sprintf("%4.0f",`Cumulative Absolute Error`),"\n","Time= ",Time,"\n",
                            Models))

ggplotcomp_models<-    
ggplot(comp_mod_df)+
  #geom_point(color="white")+
    geom_line(size=.75, aes(x = Time, y = `Cumulative Absolute Error`, color=Models,group=Models, text=text))+
    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+
  scale_fill_brewer(type="seq", palette="Greys")+
  theme_sjplot2()+
  theme(axis.text.y=element_blank(),
                    axis.text.x =  element_text(angle = 0, hjust = 1),
                    axis.ticks.y=element_blank(),                    
                    panel.grid.major.y=element_blank(),
        #panel.grid.major.x=element_blank(),
        panel.grid.minor.x=element_blank())+
  theme(legend.position='none')+
  scale_x_continuous(breaks=seq(from =1,to =max(comp_mod_df$Time)+4,by=12))+
  labs(x="Time (in weeks)", y= "Cumulative Absolute Forecast Error (CAFE)")

ggplotly(ggplotcomp_models,tooltip="text")

comp_mod_df_sum<-
comp_mod_df %>% group_by(Models) %>% summarise(mean=mean(`Cumulative Absolute Error`,na.rm=T),median=median(`Cumulative Absolute Error`,na.rm=T), p25=quantile(`Cumulative Absolute Error`,.25,na.rm=T),p75=quantile(`Cumulative Absolute Error`,.75,na.rm=T)) %>%
  dplyr::mutate(mean=scale2(mean),median=scale2(median),p25=scale2(p25),p75=scale2(p75)) %>%
  rowwise() %>%
  mutate(mean_tot = mean(c(mean,median,p25,p75))) %>% 
  dplyr::arrange(mean_tot)
  #geom_line(aes(x=Models,y=median),color="blue")+
  #geom_line(aes(x=Models,y=p25),color="green")


#impact "No counterfactual approach, No seasonal components" "Counterfactual Approach, NoSeasonal Components (impact2)" (b= SemilocalLinearTrend) (c=Gaussian, prior sd=.01) (d1= Student, Prior sd .01) (d= Student, Prior sd = .1)
#impact2d$summary[,"p"]
```

<br>

As seen in the Figure above, the errors in the different models were very similar. However, some exhibited lower cumulative errors.

<br>

```{r summary_of_models, echo=T, cache= T, paged.print=TRUE, warning=T,eval=T}
summary<-cbind(Models=c("No control variables(a)",
                          "Control variables(b)",
                          "a, LocalLinearTrend",
                          "b, LocalLinearTrend",
                          "a, LocalLinearTrend & AR",
                          "b, LocalLinearTrend & AR",                          
                          "a, Prior sd=.1",
                          "b, Prior sd=.1",
                          "a, Prior sd=.1, LocalLinearTrend",
                          "b, Prior sd=.1, LocalLinearTrend",
                          "a, Prior sd=.1, LocalLinearTrend & AR",
                          "b, Prior sd=.1, LocalLinearTrend & AR",
                          "a, Student Dist, Prior sd=.01",
                          "b, Student Dist, Prior sd=.01",
                          "a, Student Dist, Prior sd=.01, LocalLinearTrend",
                          "b, Student Dist, Prior sd=.01, LocalLinearTrend",
                          "a, Student Dist, Prior sd=.01, LocalLinearTrend & AR",
                          "b, Student Dist, Prior sd=.01, LocalLinearTrend & AR",
                          "a, Student Dist, Prior sd=.1",
                          "b, Student Dist, Prior sd=.1",
                          "a, Student Dist, Prior sd=.1, LocalLinearTrend",
                          "b, Student Dist, Prior sd=.1, LocalLinearTrend",
                          "a, Student Dist, Prior sd=.1, LocalLinearTrend & AR",
                          "b, Student Dist, Prior sd=.1, LocalLinearTrend & AR"),
               name=c("impact2","impact3","impact2b","impact3b","impact2bar","impact3bar","impact2c","impact3c","impact2c3","impact3c3","impact2c3ar","impact3c3ar","impact2d1","impact3d1","impact2d2","impact2d2ar","impact3d2","impact3d2ar","impact2d","impact3d","impact2d3","impact3d3","impact2d3ar","impact3d3ar"),
  `_`= rbind(    
                        data.table(impact2$summary,keep.rownames = T)[1,],
                        data.table(impact3$summary,keep.rownames = T)[1,],
                        data.table(impact2b$summary,keep.rownames = T)[1,],
                        data.table(impact3b$summary,keep.rownames = T)[1,],
                        data.table(impact2bar$summary,keep.rownames = T)[1,],
                        data.table(impact3bar$summary,keep.rownames = T)[1,],
                        data.table(impact2c$summary,keep.rownames = T)[1,],
                        data.table(impact3c$summary,keep.rownames = T)[1,],
                        data.table(impact2c3$summary,keep.rownames = T)[1,],
                        data.table(impact3c3$summary,keep.rownames = T)[1,],
                        data.table(impact2c3ar$summary,keep.rownames = T)[1,],
                        data.table(impact3c3ar$summary,keep.rownames = T)[1,],
                        data.table(impact2d1$summary,keep.rownames = T)[1,],
                        data.table(impact3d1$summary,keep.rownames = T)[1,],
                        data.table(impact2d2$summary,keep.rownames = T)[1,],
                        data.table(impact2d2ar$summary,keep.rownames = T)[1,],
                        data.table(impact3d2$summary,keep.rownames = T)[1,],
                        data.table(impact3d2ar$summary,keep.rownames = T)[1,],
                        data.table(impact2d$summary,keep.rownames = T)[1,],
                        data.table(impact3d$summary,keep.rownames = T)[1,],
                        data.table(impact2d3$summary,keep.rownames = T)[1,],
                        data.table(impact3d3$summary,keep.rownames = T)[1,],
                        data.table(impact2d3ar$summary,keep.rownames = T)[1,],
                        data.table(impact3d3ar$summary,keep.rownames = T)[1,]
             ))%>% data.table(.,keep.rownames = F)

comp_mod_df_sum<-
      comp_mod_df_sum %>% 
            left_join(summary, by="Models")
#comp_mod_df_sum[,c("Models","name","mean_tot")]
  summary%>%
    rename_all(~sub('_.', '', .x))%>%
    dplyr::mutate_at(4:(ncol(.)-1),~round(as.numeric(.),2)) %>% 
    dplyr::mutate_at(ncol(.),~round(as.numeric(.),4)) %>%
    #dplyr::arrange(Models,rn) %>% 
   # dplyr::rename_at(vars(starts_with('X_')),~
  knitr::kable(format = "html", format.args = list(decimal.mark = ".", big.mark = ","),
               caption="Table 2. Summary of the effects of the different models",
                 align =c('l',rep('c', 101)))%>%
   kableExtra::kable_styling(bootstrap_options = c("striped", "hover"),font_size = 8) %>%
  kableExtra::scroll_box(width = "100%", height = "375px")
```

<br>

## Poisson Distribution

<br>

We also included a Poisson regression over the raw counts of hospitalizations. **We are still trying to predict and compare these predictions to the actual values, due to computational limitations**.

<br>

```{r bsts3d_pois, echo=T, cache= T, paged.print=TRUE, warning=F,eval=F}
#The response variable (i.e., the first column in data) may contain missing values (NA), but covariates (all other columns in data) may not. If one of your covariates contains missing values, consider imputing (i.e., estimating) the missing values; if this is not feasible, leave the regressor out.

# Model 2
ss2d <- list()
# Local trend, weekly-seasonal #https://qastack.mx/stats/209426/predictions-from-bsts-model-in-r-are-failing-completely - PUSE UN GENERALIZED LOCAL TREND
ss2d <- AddLocalLevel(ss2d, data15a64_rn_causal$hosp_trauma) #
# Add weekly seasonal
ss2d <- AddSeasonal(ss2d, data15a64_rn_causal$hosp_trauma,nseasons=5, season.duration = 52) #weeks OJO, ESTOS NO SON WEEKS VERDADEROS. PORQUE TENGO MAS DE EUN AÑO
ss2d <- AddSeasonal(ss2d, data15a64_rn_causal$hosp_trauma, nseasons = 12, season.duration =4) #years
#ss2 <- AddAutoAr(ss2, y = data15a64_rn_causal$log_hosp_trauma, lags = 1) #NO PUEDO AREGAR AR1 CON POISSON
# For example, to add a day-of-week component to data with daily granularity, use model.args = list(nseasons = 7, season.duration = 1). To add a day-of-week component to data with hourly granularity, set model.args = list(nseasons = 7, season.duration = 24).


y_pois<-data15a64_rn_causal$hosp_trauma[1:(length(data15a64_rn_causal$hosp_trauma)-10)]
x1_pois<-data15a64_rn_causal$hosp_circ[1:(length(data15a64_rn_causal$hosp_circ)-10)]
x2_pois<-data15a64_rn_causal$hosp_resp[1:(length(data15a64_rn_causal$hosp_resp)-10)]
x3_pois<-data15a64_rn_causal$difftrc[1:(length(data15a64_rn_causal$difftrc)-10)]

model2d1_pois <- bsts(y_pois ~ x1_pois + x2_pois + x3_pois,
               state.specification = ss2d, #A list with elements created by AddLocalLinearTrend, AddSeasonal, and similar functions for adding components of state. See the help page for state.specification.
               family ="poisson", #A Bayesian Analysis of Time-Series Event Count Data
               niter = clus_iter,
               ping= 0,
              # burn = 500, #http://finzi.psych.upenn.edu/library/bsts/html/SuggestBurn.html Suggest the size of an MCMC burn in sample as a proportion of the total run.
               seed= 2125)
#,
#               dynamic.regression=T)
plot(model2d1_pois, main = "Model assuming Student Dist, Prior sd=.1")
plot(model2d1_pois, "components")


x_pois<-data15a64_rn[length(data15a64_rn$difftrc)-10:length(data15a64_rn$difftrc),c("hosp_trauma","hosp_circ","hosp_resp","difftrc")]

burn2d1_pois <- SuggestBurn(0.1, model2d1_pois)

#p <- predict.bsts(model2d1_pois, horizon = 10,newdata=x_pois,   burn = burn2d1_pois, quantiles = c(.025, .975))

d2 <- data.frame(
    # fitted values and predictions
    c(10^as.numeric(-colMeans(bsts.model$one.step.prediction.errors[-(1:burn),])+y),  
    10^as.numeric(p$mean)),
    # actual data and dates 
    as.numeric(AirPassengers),
    as.Date(time(AirPassengers)))
names(d2) <- c("Fitted", "Actual", "Date")
estim
```

## Selected Model

<br>

For the meantime, we selected the model with the lowest cumulative errors.

<br>

```{r bsts_model_selected2, echo=T, cache= T, paged.print=TRUE, warning=F, eval=T}
#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:
#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:
#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:
invisible(c("'0b, Student Dist, Prior sd=.01' = model2d1, #Student SD=.1 Local           'b, Student Dist, Prior sd=.1' = model2d,"))

cat("#### ",as.character(comp_mod_df_sum[1,c("Models")]),"\n")

summary(get(as.character(comp_mod_df_sum[1,c("name")]))) 

cat("#### ",as.character(comp_mod_df_sum[2,c("Models")]),"\n")
summary(get(as.character(comp_mod_df_sum[2,c("name")])))

#The “Average” column represents the average across time during the post-intervention period, while the “Cumulative” column presents the total sum of the time points. In particular, looking at the Average Absolute effect, we see that it is estimated to be 22, with a 95% posterior interval between -38 to -5. Since the interval excludes 0, we can conclude that the intervention of the Public Health Emergency in Los Angeles had a causal impact on the PM2.5 air quality with certain assumptions.
```

<br>

```{=html}
<!--- 
Generally, we can write a Bayesian structural model like this:

$$ Y_t = \mu_t + x_t \beta + S_t + e_t, e_t \sim N(0, \sigma^2_e) $$
$$ \mu_{t+1} = \mu_t + \nu_t, \nu_t \sim N(0, \sigma^2_{\nu}). $$
--->
```
<br>

```{r bsts_setting, echo=T, cache= T, paged.print=TRUE, warning=F, eval=F}

components1 = cbind.data.frame(
  colMeans(model1$state.contributions[-(1:500),"trend",]),
  colMeans(model1$state.contributions[-(1:500),"seasonal.52.1",]),
  as.Date(time(ts_pm25_wk)))
names(components1) = c("Trend", "Seasonality", "Date")
# components1 = pivot_longer(components1, cols =c("Trend","Seasonality"))
# names(components1) = c("Date", "Component", "Value")

components3 = cbind.data.frame(
  colMeans(model3$state.contributions[-(1:500),"trend",]),
  colMeans(model3$state.contributions[-(1:500),"seasonal.52.1",]),
  as.Date(time(ts_pm25_wk)))
names(components3) = c("Trend", "Seasonality", "Date")
# components3 = pivot_longer(components3, cols =c("Trend","Seasonality"))
# names(components3) = c("Date", "Component", "Value")

# Compare seasonal component of model 1 and model 3
# Model 1
plot(model1$state.specification[[2]], model1,ylim = c(-30,30),
     ylab = "Distribution", xlab = "Date")
par(new=TRUE)
plot(components1$Date, components1$Seasonality, col = "magenta", type = "l", ylim = c(-30,30)For the meantime, we selected the model with the lowest cumulative errors.,
     ylab = "Distribution", xlab = "Date")
abline(h = 10, col = "red")
abline(h = -10, col = "red")

# Model 3
plot(model3$state.specification[[2]], model3,ylim = c(-30,30),
     ylab = "Distribution", xlab = "Date")
par(new=TRUE)
plot(components3$Date, components3$Seasonality, col = "magenta", type = "l", ylim = c(-30,30),
     ylab = "Distribution", xlab = "Date")
abline(h = 10, col = "red")
abline(h = -10, col = "red")

#_#_#_#_#_#_
#_#_#_#_#_#_
# Note erroneous data outlier where pm25 was 822. Outlier of 822 was known on 2016-01-04
#dat %>%
#  filter(pm25 > 500)

components1 = cbind.data.frame(
  colMeans(model1$state.contributions[-(1:500),"trend",]),
  colMeans(model1$state.contributions[-(1:500),"seasonal.52.1",]),
  as.Date(time(ts_pm25_wk)))
names(components1) = c("Trend", "Seasonality", "Date")
# components1 = pivot_longer(components1, cols =c("Trend","Seasonality"))
# names(components1) = c("Date", "Component", "Value")

components3 = cbind.data.frame(
  colMeans(model3$state.contributions[-(1:500),"trend",]),
  colMeans(model3$state.contributions[-(1:500),"seasonal.52.1",]),
  as.Date(time(ts_pm25_wk)))
names(components3) = c("Trend", "Seasonality", "Date")
# components3 = pivot_longer(components3, cols =c("Trend","Seasonality"))
# names(components3) = c("Date", "Component", "Value")

ggplot(data=components, aes(x=Date, y=Value)) + geom_line() +
  theme_bw() + theme(legend.title = element_blank()) + ylab("") + xlab("") +
  facet_grid(Component ~ ., scales="free") + guides(colour=FALSE) +
  theme(axis.text.x=element_text(angle = -90, hjust = 0))
dev.off()
#Summing up the individual data points during the post-intervention period (which can only sometimes be meaningfully interpreted), the response variable had an overall value of 11580950511.34B. By contrast, had the intervention not taken place, we would have expected a sum of 2632428020.93B. The 95% interval of this prediction is [765079794.08B, 9222322095.11B].
```

```{r bsts4, echo=T, cache= T, paged.print=TRUE, warning=F, eval=F}
plot(model1$state.specification[[2]], model1,ylim = c(-30,30),
     ylab = "Distribution", xlab = "Date")
par(new=TRUE) 
plot(components1$Date, components1$Seasonality, col = "magenta",
     type = "l", ylim = c(-30,30), 
     ylab = "Distribution", xlab = "Date")
abline(h = 10, col = "red")
abline(h = -10, col = "red")

holiday.list <- list(NamedHoliday(holiday.name = "MemorialDay"),
                     NamedHoliday(holiday.name = "IndependenceDay"),
                     NamedHoliday(holiday.name = "LaborDay"),
                     NamedHoliday(holiday.name = "Thanksgiving"),
                     NamedHoliday(holiday.name = "Christmas"),
                     NamedHoliday(holiday.name = "NewYearsDay"))
ss <- AddRegressionHoliday(ss, data, holiday.list=holiday.list)

AddFixedDateHoliday

post.period.response <- y[post.period[1]:post.period[2],]
y <- as.numeric(data.frame(data15a64_rn[,"hosp_trauma"]))

y[post.period[1]:post.period[2]] <- NA

### Run the bsts model
ss <- AddLocalLinearTrend(list(), y)
ss <- AddSeasonal(ss, y, nseasons = 12)
bsts.model <- bsts(y, state.specification = ss, niter = 500, ping=0, seed=2016)

### Get a suggested number of burn-ins
burn <- SuggestBurn(0.1, bsts.model)

### Predict
#p <- predict.bsts(bsts.model, horizon = 12, burn = burn, quantiles = c(.025, .975))

### Actual versus predicted
d2 <- data.frame(
    # fitted values and predictions
    c(10^as.numeric(-colMeans(bsts.model$one.step.prediction.errors[-(1:burn),])+y),  
    10^as.numeric(p$mean)),
    # actual data and dates 
    as.numeric(AirPassengers),
    as.Date(time(AirPassengers)))
names(d2) <- c("Fitted", "Actual", "Date")
```

# References

-   Cox, L. (2015) Quantifying and Reducing Uncertainty about Causality in Improving Public Health and Safety. In: Ghanem R., Higdon D., Owhadi H. (eds) Handbook of Uncertainty Quantification. Springer, Cham. <https://doi.org/10.1007/978-3-319-11259-6_71-1>

-   Brodersen, KH., Gallusser, F., Koehler, J., Remy, N., Scott, SL. (2015) Inferring causal impact using Bayesian structural time-series models. Ann Appl Stat 9:247--274
